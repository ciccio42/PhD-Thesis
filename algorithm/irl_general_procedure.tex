\begin{algorithm}
\caption{Classic feature matching IRL algorithm}
\label{alg:irl}
\begin{algorithmic}
\REQUIRE Dataset of expert trajectories $\mathcal{D}^{E} = \left \{ \boldsymbol{\tau}^{E}_{i} \right \}^{N}_{i=1}$ 
\REQUIRE Reward function $R_{\omega}$, policy $\pi^{L}_{\theta}$ 
\WHILE {policy improves}
    \STATE Evaluate the state-action visitation frequency $\mu$ of the current policy $\pi^{L}_{\theta}$
    \STATE Evaluate the objective function $\mathcal{L}$, w.r.t. $\mu$ and the dataset $\mathcal{D}^{E}$
    \STATE Update the reward-function parameters $\omega$ based on $\mathcal{L}$
    \STATE Update the policy $\pi^{L}_{\theta}$ through an RL algorithm, using the updated $R_{\omega}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
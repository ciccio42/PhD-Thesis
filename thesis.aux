\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@input{chapters/abstract.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation and thesis overview}{3}{section.1.1}\protected@file@percent }
\newlabel{sec:motivation}{{1.1}{3}{Motivation and thesis overview}{section.1.1}{}}
\citation{jang2022bc_z,dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,brohan2022rt}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:welding}{{1.1a}{4}{Robots involved in arc welding operation\relax }{figure.caption.3}{}}
\newlabel{sub@fig:welding}{{a}{4}{Robots involved in arc welding operation\relax }{figure.caption.3}{}}
\newlabel{fig:material_handling}{{1.1b}{4}{Robot involved in loading operation\relax }{figure.caption.3}{}}
\newlabel{sub@fig:material_handling}{{b}{4}{Robot involved in loading operation\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Industrial Robots: example of applications\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:industrial_robots_example}{{1.1}{4}{Industrial Robots: example of applications\relax }{figure.caption.3}{}}
\citation{jang2022bc_z,yu2018daml}
\@writefile{tdo}{\contentsline {todo}{\small  Continue with organization, after prof ack.}{7}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{15467806}{20938524}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}State of the art}{7}{section.1.2}\protected@file@percent }
\newlabel{sec:sota}{{1.2}{7}{State of the art}{section.1.2}{}}
\citation{osa2018algorithmic}
\citation{osa2018algorithmic,zare2024survey}
\citation{james2018task_embedded}
\citation{jang2022bc_z}
\citation{mees2022calvin}
\citation{shridhar2023perceiver}
\citation{zhang2018deep_vr_teleoperation}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Problem formulation}{8}{subsection.1.2.1}\protected@file@percent }
\newlabel{sec:problem_formulation}{{1.2.1}{8}{Problem formulation}{subsection.1.2.1}{}}
\citation{jang2022bc_z}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{osa2018algorithmic,fang2019survey}
\citation{zhang2018deep_vr_teleoperation}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{jang2022bc_z}
\citation{kroemer2021review_robot_learning}
\citation{fang2019survey}
\citation{johns2021coarse_to_fine}
\citation{johns2021coarse_to_fine}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{caccavale2019kinesthetic,johns2021coarse_to_fine}
\citation{zhang2018deep_vr_teleoperation,mandlekar2018roboturk,jang2022bc_z,brohan2022rt,ebert22Bridge,mandlekar2023mimicgen}
\citation{dasari2020robonet,dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,chang2023one}
\citation{lee2011incremental,saveriano2015incremental}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Source of demonstration}{11}{subsection.1.2.2}\protected@file@percent }
\newlabel{sec:sod}{{1.2.2}{11}{Source of demonstration}{subsection.1.2.2}{}}
\citation{mandlekar2018roboturk,mandlekar2019scaling}
\citation{mandlekar2019scaling,mandlekar2022matters}
\citation{zhang2018deep_vr_teleoperation,jang2022bc_z,brohan2022rt}
\citation{cyberglove,touch}
\newlabel{fig:kinesthetic}{{1.2a}{12}{Example of kinesthetic teaching~\cite {johns2021coarse_to_fine}\relax }{figure.caption.5}{}}
\newlabel{sub@fig:kinesthetic}{{a}{12}{Example of kinesthetic teaching~\cite {johns2021coarse_to_fine}\relax }{figure.caption.5}{}}
\newlabel{fig:teleoperation}{{1.2b}{12}{Example of teleoperation~\cite {zhang2018deep_vr_teleoperation}\relax }{figure.caption.5}{}}
\newlabel{sub@fig:teleoperation}{{b}{12}{Example of teleoperation~\cite {zhang2018deep_vr_teleoperation}\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Examples of direct demonstration\relax }}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:direct_demonstrations}{{1.2}{12}{Examples of direct demonstration\relax }{figure.caption.5}{}}
\citation{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,chang2023one}
\citation{zhu2020robosuite}
\citation{dasari2020robonet}
\citation{liu2019_mirroring_without_overimitation}
\citation{liu2019_mirroring_without_overimitation}
\citation{smith2019avid}
\citation{smith2019avid}
\citation{nakaoka2007learning,liu2019_mirroring_without_overimitation}
\citation{nakaoka2007learning}
\citation{liu2019_mirroring_without_overimitation}
\citation{liu2017glove_force}
\newlabel{fig:wearable_indirect}{{1.3a}{14}{Example of indirect demonstration based on wearable device~\cite {liu2019_mirroring_without_overimitation}\relax }{figure.caption.7}{}}
\newlabel{sub@fig:wearable_indirect}{{a}{14}{Example of indirect demonstration based on wearable device~\cite {liu2019_mirroring_without_overimitation}\relax }{figure.caption.7}{}}
\newlabel{fig:visual_indirect}{{1.3b}{14}{Example of direct demonstration based on human video demonstration~\cite {smith2019avid}\relax }{figure.caption.7}{}}
\newlabel{sub@fig:visual_indirect}{{b}{14}{Example of direct demonstration based on human video demonstration~\cite {smith2019avid}\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Examples of indirect demonstration\relax }}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:indirect_demonstrations}{{1.3}{14}{Examples of indirect demonstration\relax }{figure.caption.7}{}}
\citation{smith2019avid,torabi2019recent_advances_lfo,xiong2021learning_by_watching,wang2023mimicplay,qian2024contrast}
\citation{liu2019_mirroring_without_overimitation}
\citation{kaelbling1996reinforcement_survey,argall2009robot_learning_from_demonstration,hussein2017imitation_learning_survey,fang2019survey,zheng2021imitation_progress_taxonomies_opportunities,zare2024survey}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Learning from demonstration}{15}{subsection.1.2.3}\protected@file@percent }
\newlabel{sec:lfd}{{1.2.3}{15}{Learning from demonstration}{subsection.1.2.3}{}}
\citation{pomerleau1988alvinn}
\citation{pomerleau1988alvinn}
\citation{ijspeert2002learning}
\@writefile{tdo}{\contentsline {todo}{\small  ADD THIS DISCUSSION}{16}{section*.10}\protected@file@percent }
\pgfsyspdfmark {pgfid2}{17302814}{17031690}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3.1}Behavioral Cloning}{16}{subsubsection.1.2.3.1}\protected@file@percent }
\newlabel{sec:bc}{{1.2.3.1}{16}{Behavioral Cloning}{subsubsection.1.2.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Taxonomy of LfD methods, divided based on type of demonstration and the learning algorithm used to learn the learner policy $\pi ^{L}$ \relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{fig:il_taxonomy}{{1.4}{17}{Taxonomy of LfD methods, divided based on type of demonstration and the learning algorithm used to learn the learner policy $\pi ^{L}$ \relax }{figure.caption.9}{}}
\newlabel{eq:bc_formula}{{1.1}{17}{Behavioral Cloning}{equation.1.2.1}{}}
\citation{ijspeert2002learning}
\citation{ijspeert2002learning,ijspeert2013dynamical}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Abstract Algorithm for BC methods\relax }}{18}{algorithm.1}\protected@file@percent }
\newlabel{alg:bc}{{1}{18}{Abstract Algorithm for BC methods\relax }{algorithm.1}{}}
\newlabel{eq:dmp}{{1.2}{18}{Dynamical Movement Primitives}{equation.1.2.2}{}}
\citation{ijspeert2002learning}
\citation{si2023composite}
\citation{li2023human}
\citation{fanger2016gaussian}
\newlabel{eq:dmp_loss}{{1.3}{19}{Dynamical Movement Primitives}{equation.1.2.3}{}}
\citation{paraschos2013ProMPs}
\citation{saveriano2023dynamic}
\citation{zhou2019learning}
\citation{zhou2019learning}
\citation{zhou2019learning}
\newlabel{formula:vmp_elementary_trajectory_linear_velocity}{{1.4}{21}{Dynamical Movement Primitives}{equation.1.2.4}{}}
\newlabel{formula:vmp_elementary_minimum_jerk}{{1.5}{21}{Dynamical Movement Primitives}{equation.1.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Graphical representation of the idea behind VMP \cite  {zhou2019learning}. The final trajectory \( y \) is represented as the sum of two components: the elementary trajectory \( h \), and the shape modulation \( f \). The elementary trajectory can directly connect two points (e.g., start and goal points) with a linear segment.\relax }}{22}{figure.caption.12}\protected@file@percent }
\newlabel{fig:vmp}{{1.5}{22}{Graphical representation of the idea behind VMP \cite {zhou2019learning}. The final trajectory \( y \) is represented as the sum of two components: the elementary trajectory \( h \), and the shape modulation \( f \). The elementary trajectory can directly connect two points (e.g., start and goal points) with a linear segment.\relax }{figure.caption.12}{}}
\citation{meier2011movement_primitive,caccavale2019kinesthetic,agostini2020manipulation}
\citation{meier2011movement_primitive}
\citation{caccavale2019kinesthetic}
\citation{pomerleau1988alvinn}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{mandlekar2022matters}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Architecture proposed in\nobreakspace  {}\cite  {zhang2018deep_vr_teleoperation}\relax }}{25}{figure.caption.14}\protected@file@percent }
\newlabel{fig:deep_bc}{{1.6}{25}{Architecture proposed in~\cite {zhang2018deep_vr_teleoperation}\relax }{figure.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Statistics of Training set, and Test Success rate\nobreakspace  {}\cite  {zhang2018deep_vr_teleoperation}\relax }}{25}{table.caption.15}\protected@file@percent }
\newlabel{table:deep_vr_teleoperation_results}{{1.1}{25}{Statistics of Training set, and Test Success rate~\cite {zhang2018deep_vr_teleoperation}\relax }{table.caption.15}{}}
\citation{mandlekar2022matters}
\citation{mandlekar2022matters}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces The set of tasks presented in the benchmark \cite  {mandlekar2022matters}.\relax }}{26}{figure.caption.16}\protected@file@percent }
\newlabel{fig:what_matters_task}{{1.7}{26}{The set of tasks presented in the benchmark \cite {mandlekar2022matters}.\relax }{figure.caption.16}{}}
\citation{shafiullah2022behavior}
\citation{nair2022r3m,cheng2023diffusion,shi2023waypoint}
\citation{nair2022r3m}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Results are presented on tasks performed in a high-dimensional observation space for simulated environments. PH refers to \textit  {Proficient Human}, which represents trajectories collected by a single expert human demonstrator with extensive experience in teleoperating the robot. MH refers to \textit  {Multi Human}, which represents trajectories collected by multiple human operators with varying levels of expertise in teleoperation.\relax }}{27}{table.caption.17}\protected@file@percent }
\newlabel{table:what_matters_res_low_dimensional}{{1.2}{27}{Results are presented on tasks performed in a high-dimensional observation space for simulated environments. PH refers to \textit {Proficient Human}, which represents trajectories collected by a single expert human demonstrator with extensive experience in teleoperating the robot. MH refers to \textit {Multi Human}, which represents trajectories collected by multiple human operators with varying levels of expertise in teleoperation.\relax }{table.caption.17}{}}
\citation{nair2022r3m}
\citation{grauman2022ego4d}
\citation{resnet}
\citation{cheng2023diffusion}
\citation{cheng2023diffusion}
\citation{perez2018film}
\citation{cheng2023diffusion}
\citation{perez2018film}
\newlabel{eq:denoising}{{1.6}{29}{Single-Task Imitation Learning}{equation.1.2.6}{}}
\citation{shi2023waypoint}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Architecture presented in \cite  {cheng2023diffusion}. (a) General formulation, at time step $t$, the policy inputs the latest $T_o$ steps of observation data $O_t$ and outputs $T_a$ steps of actions $A_t$. (b) CNN-based Diffusion Policy, the observation feature $O_t$ is conditioned using FiLM \cite  {perez2018film}. Starting with $A_t^K$ from Gaussian noise, the noise-prediction network $\epsilon _\theta $ iteratively subtracts noise to obtain the denoised action sequence $A_t^0$. (c) Transformer-based Diffusion Policy, the observation embedding $O_t$ is fed into a multi-head cross-attention layer within each decoder block, with causal attention applied to constrain each action embedding to attend only to itself and prior actions.\relax }}{30}{figure.caption.18}\protected@file@percent }
\newlabel{fig:diffusion_model}{{1.8}{30}{Architecture presented in \cite {cheng2023diffusion}. (a) General formulation, at time step $t$, the policy inputs the latest $T_o$ steps of observation data $O_t$ and outputs $T_a$ steps of actions $A_t$. (b) CNN-based Diffusion Policy, the observation feature $O_t$ is conditioned using FiLM \cite {perez2018film}. Starting with $A_t^K$ from Gaussian noise, the noise-prediction network $\epsilon _\theta $ iteratively subtracts noise to obtain the denoised action sequence $A_t^0$. (c) Transformer-based Diffusion Policy, the observation embedding $O_t$ is fed into a multi-head cross-attention layer within each decoder block, with causal attention applied to constrain each action embedding to attend only to itself and prior actions.\relax }{figure.caption.18}{}}
\citation{cheng2023diffusion}
\citation{mandlekar2022matters}
\citation{cheng2023diffusion}
\newlabel{eq:awe}{{1.7}{31}{Single-Task Imitation Learning}{equation.1.2.7}{}}
\citation{pomerleau1988alvinn}
\citation{ross2010efficient_reductions}
\citation{ross2011dagger}
\citation{laskey2017comparing_hc_rc}
\citation{ross2011dagger}
\citation{ross2011dagger}
\citation{kelly2019hg_dagger}
\citation{jang2022bc_z}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces DAgger Algorithm \cite  {ross2011dagger}\relax }}{33}{algorithm.2}\protected@file@percent }
\newlabel{alg:dagger}{{2}{33}{DAgger Algorithm \cite {ross2011dagger}\relax }{algorithm.2}{}}
\citation{mandlekar2020human_in_the_loop,chisari2022correct}
\citation{chisari2022correct}
\citation{james2018task_embedded,bhutani2022attentive_one_shot,dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}
\citation{stepputtis2020language,jang2022bc_z,mees2022calvin,doasIcan2022,mees2022hulc,brohan2022rt,shridhar2023perceiver}
\citation{jiang2023vima}
\newlabel{eq:mse}{{1.8}{35}{Multi-Task Imitation Learning}{equation.1.2.8}{}}
\citation{finn2017maml}
\citation{finn2017maml}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Multi-Task Imitation Learning Taxonomy\relax }}{36}{figure.caption.21}\protected@file@percent }
\newlabel{fig:mtil_taxonomy}{{1.9}{36}{Multi-Task Imitation Learning Taxonomy\relax }{figure.caption.21}{}}
\newlabel{eq:nll}{{1.9}{36}{Multi-Task Imitation Learning}{equation.1.2.9}{}}
\citation{finn2017maml}
\citation{finn2017maml}
\citation{finn2017one_shot_visual_il,yu2018daml,yu2018one_shot_hil}
\citation{finn2017one_shot_visual_il}
\citation{duan2017one_shot_il}
\citation{duan2017one_shot_il}
\citation{yu2018daml}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Model-Agnostic Meta-Learning (MAML) \cite  {finn2017maml}\relax }}{38}{algorithm.3}\protected@file@percent }
\newlabel{alg:maml}{{3}{38}{Model-Agnostic Meta-Learning (MAML) \cite {finn2017maml}\relax }{algorithm.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Diagram of MAML algorithm, which optimizes for a representation $\theta $ that can quickly adapt to new tasks\relax }}{39}{figure.caption.22}\protected@file@percent }
\newlabel{fig:maml_weights}{{1.10}{39}{Diagram of MAML algorithm, which optimizes for a representation $\theta $ that can quickly adapt to new tasks\relax }{figure.caption.22}{}}
\newlabel{eq:daml}{{1.10}{39}{Multi-Task Imitation Learning}{equation.1.2.10}{}}
\citation{yu2018daml}
\citation{yu2018daml}
\newlabel{eq:daml_temporal_adaptation_loss}{{1.11}{40}{Multi-Task Imitation Learning}{equation.1.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Tasks performed in \cite  {yu2018daml}. (Top row) Human demonstration, (Bottom row) robot demonstration. (Left) Placing task, (Middle) pushing task, (Right) pick-and-place task.\relax }}{41}{figure.caption.23}\protected@file@percent }
\newlabel{fig:daml_tasks}{{1.11}{41}{Tasks performed in \cite {yu2018daml}. (Top row) Human demonstration, (Bottom row) robot demonstration. (Left) Placing task, (Middle) pushing task, (Right) pick-and-place task.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces The Temporal Adaptation Loss architecture applies 1D temporal convolutional layers to the stacked embeddings generated by the policy $\pi $ from the frames of the human video demonstration.\relax }}{41}{figure.caption.24}\protected@file@percent }
\newlabel{fig:daml_temporal_adaptation_loss}{{1.12}{41}{The Temporal Adaptation Loss architecture applies 1D temporal convolutional layers to the stacked embeddings generated by the policy $\pi $ from the frames of the human video demonstration.\relax }{figure.caption.24}{}}
\citation{stepputtis2020language}
\citation{jang2022bc_z}
\citation{stepputtis2020language}
\citation{fastrcnn}
\citation{pennington2014glove}
\citation{stepputtis2020language}
\citation{stepputtis2020language}
\citation{stepputtis2020language}
\citation{stepputtis2020language}
\citation{jang2022bc_z}
\citation{kelly2019hg_dagger}
\citation{perez2018film}
\citation{jang2022bc_z}
\citation{jang2022bc_z}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Architecture proposed in \cite  {stepputtis2020language}. The \textit  {Semantic Model} takes in input the image $I$ and the command $v$, generating a command conditioned embedding $e$. The \textit  {Control Module} receives in input the embedding $e$ and the current robot state $r_{t}$ and produces the next control signal.\relax }}{44}{figure.caption.25}\protected@file@percent }
\newlabel{fig:language_conditioned}{{1.13}{44}{Architecture proposed in \cite {stepputtis2020language}. The \textit {Semantic Model} takes in input the image $I$ and the command $v$, generating a command conditioned embedding $e$. The \textit {Control Module} receives in input the embedding $e$ and the current robot state $r_{t}$ and produces the next control signal.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces (Left) Set of object used in \cite  {stepputtis2020language}. (Right) Sample of task execution (right)\relax }}{44}{figure.caption.26}\protected@file@percent }
\newlabel{fig:objects}{{1.14}{44}{(Left) Set of object used in \cite {stepputtis2020language}. (Right) Sample of task execution (right)\relax }{figure.caption.26}{}}
\citation{brohan2022rt,mees2022calvin,mees2022hulc}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{jang2022bc_z}
\citation{tan2019efficientnet}
\citation{cer2018universal}
\citation{ryoo2021tokenlearner}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Architecture proposed in \cite  {jang2022bc_z}. Here, the Task Embedding is injected directly in the Feature Maps generated by the ResNet-18.\relax }}{45}{figure.caption.27}\protected@file@percent }
\newlabel{fig:bcz_architecture}{{1.15}{45}{Architecture proposed in \cite {jang2022bc_z}. Here, the Task Embedding is injected directly in the Feature Maps generated by the ResNet-18.\relax }{figure.caption.27}{}}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{grill2003neural}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Examples of household scenarios in RT-1 large-scale dataset.\relax }}{46}{figure.caption.28}\protected@file@percent }
\newlabel{fig:rt_1_dataset}{{1.16}{46}{Examples of household scenarios in RT-1 large-scale dataset.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces RT-1 Language-Conditioned Transformer based architecture proposed in \cite  {brohan2022rt}.\relax }}{46}{figure.caption.29}\protected@file@percent }
\newlabel{fig:rt_1_model}{{1.17}{46}{RT-1 Language-Conditioned Transformer based architecture proposed in \cite {brohan2022rt}.\relax }{figure.caption.29}{}}
\citation{brohan2022rt}
\citation{brohan2022rt}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Distribution of tasks in large-scale dataset proposed in \cite  {brohan2022rt}\relax }}{47}{table.caption.30}\protected@file@percent }
\newlabel{table:rf1_dataset}{{1.3}{47}{Distribution of tasks in large-scale dataset proposed in \cite {brohan2022rt}\relax }{table.caption.30}{}}
\citation{mees2022calvin}
\citation{calvin}
\citation{calvin}
\citation{mees2022hulc,mees2023hulc++,reuss2024multimodal}
\citation{reuss2024multimodal}
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces Results reported in \cite  {brohan2022rt} by training the same model RT-1 with different dataset size\relax }}{48}{table.caption.31}\protected@file@percent }
\newlabel{table:rt1_results}{{1.4}{48}{Results reported in \cite {brohan2022rt} by training the same model RT-1 with different dataset size\relax }{table.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces Environment proposed in CALVIN benchmark \cite  {calvin}. The environments have different textures and all static elements such as the sliding door, the drawer, the light button, and switch are positioned differently\relax }}{48}{figure.caption.32}\protected@file@percent }
\newlabel{fig:calvin_env}{{1.18}{48}{Environment proposed in CALVIN benchmark \cite {calvin}. The environments have different textures and all static elements such as the sliding door, the drawer, the light button, and switch are positioned differently\relax }{figure.caption.32}{}}
\citation{reuss2024multimodal}
\citation{reuss2024multimodal}
\citation{shridhar2022cliport,shridhar2023perceiver}
\citation{shridhar2022cliport}
\citation{radford2021learning}
\citation{zeng2021transporter}
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces The Multimodal Diffusion Transformer (MDT) architecture proposed in \cite  {reuss2024multimodal}\relax }}{50}{figure.caption.33}\protected@file@percent }
\newlabel{fig:mdt_architecture}{{1.19}{50}{The Multimodal Diffusion Transformer (MDT) architecture proposed in \cite {reuss2024multimodal}\relax }{figure.caption.33}{}}
\citation{shridhar2022cliport}
\citation{shridhar2022cliport}
\citation{james2018task_embedded}
\citation{bhutani2022attentive_one_shot}
\citation{james2018task_embedded}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces The Dual Stream architecture proposed in \cite  {shridhar2022cliport} consists of two parallel streams: a semantic stream and a spatial stream. The semantic stream utilizes a frozen CLIP ResNet50 to encode the RGB input, with the decoder layers conditioned by tiled language features from the CLIP sentence encoder. Meanwhile, the spatial stream encodes the RGB-D input, and its decoder layers are laterally fused with those of the semantic stream. The final output is a map of dense pixelwise features, which is used to predict pick or place affordances.\relax }}{52}{figure.caption.34}\protected@file@percent }
\newlabel{fig:clip_port_architecture}{{1.20}{52}{The Dual Stream architecture proposed in \cite {shridhar2022cliport} consists of two parallel streams: a semantic stream and a spatial stream. The semantic stream utilizes a frozen CLIP ResNet50 to encode the RGB input, with the decoder layers conditioned by tiled language features from the CLIP sentence encoder. Meanwhile, the spatial stream encodes the RGB-D input, and its decoder layers are laterally fused with those of the semantic stream. The final output is a map of dense pixelwise features, which is used to predict pick or place affordances.\relax }{figure.caption.34}{}}
\citation{james2018task_embedded}
\citation{james2018task_embedded}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces Example of affordance map. (Left) The top-view input image. (Center) The affordance map for the pick-operation, since the task is to grab cherries the map highlights the two pickable cherries. (Right) The affordance map for the place operation.\relax }}{53}{figure.caption.35}\protected@file@percent }
\newlabel{fig:clip_port_affordance}{{1.21}{53}{Example of affordance map. (Left) The top-view input image. (Center) The affordance map for the pick-operation, since the task is to grab cherries the map highlights the two pickable cherries. (Right) The affordance map for the place operation.\relax }{figure.caption.35}{}}
\newlabel{eq:task_loss}{{1.12}{53}{Multi-Task Imitation Learning}{equation.1.2.12}{}}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{resnet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Architecture proposed in \cite  {james2018task_embedded}. The \textit  {Task Embedding Net} generates the embedding representing the task to be performed given the goal image. The \textit  {Control Net} implements the policy, and takes in input the current observation and the tiled task embedding\relax }}{54}{figure.caption.36}\protected@file@percent }
\newlabel{fig:task_embedded}{{1.22}{54}{Architecture proposed in \cite {james2018task_embedded}. The \textit {Task Embedding Net} generates the embedding representing the task to be performed given the goal image. The \textit {Control Net} implements the policy, and takes in input the current observation and the tiled task embedding\relax }{figure.caption.36}{}}
\citation{vaswani2017attention}
\citation{dasari2021transformers_one_shot}
\citation{dasari2021transformers_one_shot}
\newlabel{eq:tosil_bc}{{1.13}{55}{Multi-Task Imitation Learning}{equation.1.2.13}{}}
\newlabel{eq:tosil_inv}{{1.14}{55}{Multi-Task Imitation Learning}{equation.1.2.14}{}}
\citation{dasari2021transformers_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces Transformer based architecture proposed in \cite  {dasari2021transformers_one_shot}. The Transformer network is used to create task-specific representation, given context and observation features computed with ResNet-18.\relax }}{56}{figure.caption.37}\protected@file@percent }
\newlabel{fig:tosil_architecture}{{1.23}{56}{Transformer based architecture proposed in \cite {dasari2021transformers_one_shot}. The Transformer network is used to create task-specific representation, given context and observation features computed with ResNet-18.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces Validation setting proposed in \cite  {dasari2021transformers_one_shot}. The 16 tasks consist of taking an object (a-b) to a bin (1-4). On the left there is the demonstrator rob
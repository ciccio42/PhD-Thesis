\paragraph*{Multi-Task Imitation Learning}\mbox{}\\
The methods discussed in the previous paragraphs describe architectures and approaches specifically designed to solve a single task, with limited generalization to the object's category (e.g., picking blocks rather than balls) and initial state (e.g., the object's starting position). For instance, a system trained for pick-and-place operations cannot be repurposed for tasks like assembly operations. The methods described in this paragraph address these limitations, solving the problem of \textit{Multi-Task Imitation Learning} (MTIL)

Before starting to present and describe the different methods and approaches, it is necessary to describe the problem. Starting from a reformulation of the dataset used to train the system and the learned policy.
Indeed, in Section~\ref{sec:problem_formulation}, the expert dataset $\mathcal{D}^{E}$ has been introduced. Based on the problem to solve this dataset can composed in different way. Specifically, in the context of MTIL, the dataset $\mathcal{D}^{E}$ can be seen as a composition of $n$ datasets, $\mathcal{D}^{E}=\left \{\mathcal{D}_{1}, \dots, \mathcal{D}_{n}\right \}$, where $\mathcal{D}_{i} = \left \{ (\tau_{m_{i}}^{j}, \ c_{m_{i}}), j=1,\dots,N, \ m_{i} \in \mathcal{M}_{i}\right \}$ is the \textit{single-task dataset}, composed of:
\begin{itemize}
    \item $N$ expert demonstration for each $m^{th}$ variation of the $i^{th}$ task, where $M_{i}$ is the number of variation for the $i^{th}$ task.
    \item Agent trajectories $\tau_{m_{i}}^{j} = (s_{0}, a_{0}, s_{1}, a_{1}, \dots, a_{T-1}, s_{T})$. where $s_{t}$ is the state at time $t$ and $a_{t}$ is the corresponding action (Section \ref{sec:problem_formulation}).
    \item Task-conditioning signal $c_{m_{i}}$ for the $m^{th}$ variation of $i^{th}$ task, which describes the desired task in terms of video demonstrations \cite{james2018task_embedded,bhutani2022attentive_one_shot,dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}, natural language description \cite{stepputtis2020language,jang2022bc_z,mees2022calvin,doasIcan2022,mees2022hulc,brohan2022rt,shridhar2023perceiver} or multi-modal prompt, that exploits both visual information (e.g., an image of the target object) and text information that contains the information related to the action to be performed \cite{jiang2023vima}.
\end{itemize}
The goal of Multi-Task Imitation Learning is to learn a \textit{conditioned policy} $\pi^{L}_{\theta}(a_{t}|s_{t}, c_{m_{i}})$, that is able to map the current state and command into the corresponding action.
Depending on how the policy is defined, various loss functions come into play. In the case of deterministic policies, the learning process focuses on minimizing the Mean-Squared Error (refer to Formula \ref{eq:mse}). However, for probabilistic policies, the learning process centers around minimizing the Negative Log-likelihood (refer to Formula \ref{eq:nll}). This approach aims to enhance the probability of correctly executing the action.
\input{formula/loss_functions.tex}
The following sections will describe the various approaches proposed to address the problem. Figure \ref{fig:mtil_taxonomy} illustrates the taxonomy used for the Multi-Task Imitation Learning methods. Specifically, the methods are categorized based on the type of generalization required by the algorithm (Few-Shot vs. Zero-Shot). For Few-Shot generalization, the Meta-Learning paradigm will be discussed, as it is most relevant to the problem at hand. For Zero-Shot generalization, the methods are further divided based on the type of conditioning signal used, whether it is provided through natural language descriptions or visual information.
\input{figures/ch1/mtil_taxonomy.tex}

\textbf{Few-Shot MTIL} refers to approaches designed to train a model on a variety of tasks so that it can effectively solve a new task using only a small number of samples and consequently requires only a few back-propagation steps \cite{finn2017maml}. In this context, one of the most significant learning paradigms, especially relevant to robotic manipulation, is \textit{Meta-Learning}. The goal of Meta-Learning is to train a model that can ``learn to learn," meaning it develops a set of general weights $\theta$ that, while not directly usable for solving any specific task within a distribution of tasks $\mathcal{T}$, can be quickly adapted through a few backpropagation steps to solve a given task within that distribution, $\mathcal{T}_{i} \in \mathcal{T}$. One of the most popular Meta-Learning algorithms is the \textit{Model-Agnostic Meta-Learning} (MAML) algorithm \cite{finn2017maml}, described in Algorithm \ref{alg:maml}. The MAML algorithm follows an iterative learning procedure consisting of two steps:
\begin{itemize}
    \item \textbf{Meta-Learning}: During this phase, task-specific weights $\theta_{i}$ are computed for each sampled task $\mathcal{T}_{i}$. Specifically, the \textit{meta-parameters} $\theta$ are updated according to the gradient obtained from evaluating the loss function on the $i^{th}$ task $\mathcal{T}_{i}$, where the function $f$ is parameterized by the meta-parameters $\theta$.

    \item \textbf{Meta-Adaptation}: In this phase, the meta-parameters are further refined. The loss function $f$, now parameterized by the task-specific parameters for the $i^{th}$ task, is used to adjust the meta-parameters based on the gradients derived from the sum of the loss functions evaluated on the task-specific weights. This process provides feedback to the meta-parameters $\theta$ from each task, leading to a generalized point that can be easily adapted to new tasks (Figure \ref{fig:maml_weights}).
\end{itemize}
\input{algorithm/maml.tex}
\input{figures/ch1/maml_weigths.tex}
The MAML algorithm is the base for different methods which apply Few-Shot Imitation Learning in the context of Behavioral Cloning \cite{finn2017one_shot_visual_il,yu2018daml,yu2018one_shot_hil}.

In \cite{finn2017one_shot_visual_il}, MAML algorithm was used to prove the effectiveness of Meta-Learning in the context of real robot manipulation, with visual observations, as opposite to \cite{duan2017one_shot_il}. A Convolutional Neural Network was trained by following the Algorithm \ref{alg:maml}, using as loss-function the Mean Squared Error, computed between the predicted action and the ground truth one. For real-robot experiments a dataset of \textbf{1300} placing demonstrations (i.e., place an holded object in a target container), containing near to \textbf{100} different objects, was collected through teleportation. The trained system was tested by performing the adaptation step on one video demonstration, over 29 new objects, moreover, between the video demonstration and the actual execution, the objects configuration was changed. In this setting the system reached the $\mathbf{90\%}$ of success rate, outperforming baseline methods based on LSTM \cite{duan2017one_shot_il}, and contextual network (i.e., a Convolutional Neural Network that takes in input the current observation and the image representing the target state).

In \cite{yu2018daml}, the \textit{Domain Adaptive Meta-Learning} algorithm (DAML) was proposed with the goal of learning to infer a policy from a single human demonstration. To achieve it, a two-step algorithm was proposed. In the first-step, called \textbf{Meta-Learning step}, given in input, for each task $\mathcal{T}$, a set of human demo $\mathcal{D}^{h}_{\mathcal{T}}$ and a set or robot demo $\mathcal{D}^{r}_{\mathcal{T}}$ (Figure \ref{fig:daml_tasks}), the \textit{initial policy parameters} $\theta$ and the \textit{adaptive loss} parameters $\psi$ are learned, solving the problem in Formula \ref{eq:daml}.
\input{formula/daml.tex}
\newline The outer loss is the classic supervised Behavioral Cloning loss, defined as $\mathcal{L}_{BC}(\phi, \mathbf{d^{r}}) = \sum_{t} \log(\pi_{\phi}(a_{t} \mid s_{t}, o_{t}))$. The inner loss, $\mathcal{L}_{\psi}$, is a learned \textbf{adaptive loss}. Specifically, $\mathcal{L}_{\psi}$ is used during Meta-Adaptation, where the policy parameters are updated by evaluating the gradients derived from $\mathcal{L}_{\psi}$. This process involves using a video of a human demonstrating a new task $\mathcal{T}$ as input, leading to the policy update defined by $\phi_{\mathcal{T}} = \theta - \alpha \nabla_{\theta} \mathcal{L}_{\psi}(\theta, \mathbf{d}^{h})$. 
\newline This adaptive loss is the key component proposed in DAML. To use it effectively, it is necessary to learn the parameters $\psi$, observing how there is no direct correspondence between the human video demonstration and the robot's ground truth actions. To address this challenge, the authors of DAML observed that while the policy learns to produce appropriate actions through the $\mathcal{L}_{BC}$ loss, the adaptive loss should instead adjust the perceptual aspect of the policy, focusing on human motion and the manipulated object. Based on this insight, the authors implemented the function $\mathcal{L}_{\psi}$ using a 1D Temporal Convolutional Network (Figure \ref{fig:daml_temporal_adaptation_loss}). The convolutional layers are applied to a stack of embeddings generated by the policy $\pi$ across different frames of the video demonstrations. The parameters of this module are learned during the meta-training phase, following the weight update process described in Formula \ref{eq:daml_temporal_adaptation_loss}. The objective of $\mathcal{L}_{\psi}$ is to generate task-specific policy parameters $\phi_{\mathcal{T}}$ that guide the policy to produce effective actions.

\input{formula/daml_temporal_adaptation_loss.tex}
\newline Experimental evaluation on tasks such as placing, pushing, and pick-and-place, has shown that: \begin{itemize}
    \item The system was able to generalize across both new objects and objects configuration starting from only a single human demonstration;
    \item A performance degradation was observed in large domain-shift experiments, such as novel backgrounds and different camera view-points.
\end{itemize}
\input{figures/ch1/daml.tex}

Meta-Learning algorithms have demonstrated intriguing properties, notably their capacity for few-shot generalization to novel objects and object configurations. However, it has been observed that during the adaptation step, these methods tend to lose their effectiveness in performing other tasks. This limitation has underscored the need for the development of Multi-Task Imitation Learning methods, which aim to address these shortcomings and enable more versatile task execution in complex scenarios. These kind of methods will be discussed in the following paragraphs.

\textbf{Zero-Shot MTIL} refers to approaches that aim to train a model capable of solving different tasks without any further adaptation or backpropagation steps. This approach addresses a key issue in Meta-Learning methods, which is the problem of forgetting how to solve previous tasks after adapting to a new one. The goal is to develop a single policy that can handle multiple tasks in a zero-shot manner.

In this context, a crucial design choice is how to convey the desired task to the policy. The literature identifies two main methods to address this problem:

\begin{enumerate}
    \item \textit{Language Conditioned}: These methods leverage natural language descriptions of tasks to inform the model about the task to be executed.
    \item \textit{Visual Conditioned}: These methods use visual information (e.g., goal-state images, video demonstrations) to provide the model with the task instructions.
\end{enumerate}
\textcolor{red}{ToDo}

\textit{Language Conditioned}, as said a possible and intuitive way to inform the policy about which task to execute is through natural language description. Indeed, by looking to the phrase ``Pick the blue cube and place it in the red bow'', there are both information about the action to perform (pick and place) and which are the objects involved (blue cude and red bow). Consequently, through training a neural network with a diverse set of tasks, the system should exhibit the ability to generalize its understanding to both previously unseen objects within familiar tasks and entirely novel tasks composed of the fundamental actions practiced during training. This approach showcases the potential for \textbf{robust} and \textbf{adaptable} \textbf{human-robot interaction} in real-world scenarios.

Foundational research, such as that by \cite{stepputtis2020language} and \cite{jang2022bc_z}, has sought to explore the previously mentioned hypothesis. Notably, \cite{stepputtis2020language} introduced an innovative architectural framework, depicted in Figure \ref{fig:language_conditioned}, marking the first instance of seamlessly integrating language, vision, and control tasks. This model is composed of two critical components: a \textit{Semantic Model}, which generates a task embedding denoted as $e$ by processing the initial scene image and the accompanying text command, and a \textit{Control Model}, which generates the control signal using the current robot state $r_{t}$ and the task embedding $e$.

A crucial aspect of such architectures is the management of the visual state, represented by the image $I$, and the command $v$, to create a meaningful embedding $e$ that encapsulates both the current scene state and the desired command. Specifically, the image is first processed using a pre-trained object detection network (Faster R-CNN \cite{fastrcnn}) to identify salient objects in the robot's environment. The detected objects are represented by feature vectors, which include class and bounding box information. Concurrently, the language command is embedded into a suitable representation using a language embedding technique (e.g., GloVe \cite{pennington2014glove}), with the command vector $V$ encoded by a recurrent GRU unit. To associate the objects identified with the sentence embedding $s$, a likelihood value is computed for each object proposal $a_{i} = w_{a}^{T} f_{a}([\text{f}_{i}, s])$, and a probability distribution is computed over the candidates $\mathbf{a} = \text{softmax}([a_0, \dots, a_c])$. Finally, the task embedding $e$ is formed by a fully connected layer that takes as input the sentence embedding $s$ and the weighted sum of object candidate features $e'= \sum_{i=0}^{c} f_{i}a_{i}$.

Training of this model was conducted on two fundamental tasks, namely ``Picking" and ``Pouring", within scenarios featuring multiple objects of the same category, which served as distractors (see Figure \ref{fig:objects}). The subsequent testing experiments demonstrated the system's capability to successfully complete the picking task 98 out of 100 times and the pouring task 85 out of 100 times within novel scenarios. These results serve as compelling evidence of the efficacy and potential of language-conditioned methodologies in the field.
\input{figures/ch1/language_conditioned.tex}

Authors in \cite{jang2022bc_z} make a step towards a more general agent, by proposing a large-scale dataset containing \textbf{100} diverse manipulation tasks. The demonstrations were collected through both expert teleoperation and shared autonomy process (HG-DAgger \cite{kelly2019hg_dagger}). The demonstrated tasks were related to pick-and-place, grasp, pick-and-drag, pick-and-wipe, and push skills. The dataset was used to train the network in Figure \ref{fig:bcz_architecture}. As it can be noted the samples were composed by the current robot observation, and a conditioning represented by either a \textit{\textbf{natural language description}} or a \textit{\textbf{human video demonstration}}.
The idea was that training a conditioned policy over the current observation $o_{t}$, and a task representation $c$, , it would allow the policy to generalize over new tasks in a zero-shot manner (i.e., without any fine-tuning). 
Specifically, in contrast to the previous method, this approach does not rely on pre-trained object detectors to identify candidate regions. Instead, the Task Embedding is directly injected into the Feature Maps generated by the Convolutional Neural Network ResNet-18. This injection is carried out through the FiLM layer \cite{perez2018film}.

Experimental results shown that, over 28 held-out tasks, containing both completely new objects, and known objects but in different tasks, an average success rate of \textbf{38\%} was reached in the easiest setting, with only one distractor and with natural language instruction. The success rate dropped to \textbf{4\%} in the hardest setting with 4 distractors and video conditioning.
\input{figures/ch1/bc-z.tex}

Foundational research in the field of Language-Conditioned Multi-Task Imitation Learning has demonstrated promising results in zero-shot generalization. However, the robustness of their performance remains a challenge. Subsequent research, as highlighted in \cite{brohan2022rt,mees2022calvin,mees2022hulc}, has focused on enhancing performance. 

In particular, the authors of \cite{brohan2022rt} sought to investigate whether the transfer of knowledge from extensive, diverse, and task-agnostic datasets, which has enabled modern machine learning models to excel in zero or few-shot learning scenarios for new and specific tasks, is applicable within the realm of robotics. This inquiry arises due to the presence of high-capacity architectures capable of assimilating knowledge from such large datasets. To explore this prospect, the authors in \cite{brohan2022rt} introduced a comprehensive dataset comprising over 130,000 demonstrations collected across more than 700 household tasks (Figure \ref{fig:rt_1_dataset}). Additionally, they proposed a Language-Conditioned Transformer-based architecture (Figure \ref{fig:rt_1_model}). Here the authors made relevant modification in the architecture, with respect to the previous BC-Z architecture \cite{jang2022bc_z}.Indeed, they modified the Visual Module by using an EfficientNet \cite{tan2019efficientnet} instead of the ResNet-18. The instruction was encoded using the Universal Sentence Encoder \cite{cer2018universal}, and the policy was implemented through a Transformer. Additionally, the authors addressed the real-time constraints of robot control. To accelerate inference time and achieve a frequency of $3$ Hz, the authors employed a TokenLearner module \cite{ryoo2021tokenlearner}, which utilizes an attention mechanism to select the most relevant tokens, thereby reducing the number of tokens that the underlying control module must process to infer the action.
\input{figures/ch1/rt1_model.tex}
\input{tables/ch1/rt_1_dataset.tex}
It is worth noting the intriguing findings presented in Table \ref{table:rt1_results}. The model exhibits robustness in replicating tasks it has encountered before, and it even performs reasonably well on tasks it hasn't seen previously. However, a notable decline in performance becomes apparent when the model is exposed to novel backgrounds and scenarios featuring distracting objects, especially when the available data for these situations is limited. This observed trend holds significance because, unlike domains such as Computer Vision and Natural Language Processing where gathering large-scale datasets is relatively straightforward, the collection of real-world robotic datasets is a laborious and time-consuming endeavor. Furthermore, these real-world robotic datasets often have limited applicability to other research due to \textbf{disparities in action space, robot morphology, and scene representation}, as pointed out by \cite{brohan2022rt}.Therefore, the aspiration is to develop a system capable of replicating tasks with minimal demonstrations specifically gathered from the particular robot in use.
\newline Furthermore, the decline in success performance when faced with distractor objects emphasizes that addressing the policy-learning problem in an end-to-end manner, which involves mapping high-dimensional and high-level inputs like images and text directly to low-dimensional, low-level outputs such as the actions to be executed, may not be the most effective approach. This is because such models might lack the necessary perceptual components that enable them to initially recognize the target object within the scene, subsequently navigate towards it, and commence the manipulation process. This process aligns with how humans approach manipulation tasks \cite{grill2003neural}.
\input{tables/ch1/rt_1_results.tex}

As we have seen up to now, all the proposed methods were tested on different robotic platforms with different scenarios and environments. As in other Computer Vision problem, there are no well now benchmark that are used by the researchers around the world. To solve this problem, authors in \cite{mees2022calvin} proposed CALVIN (Composing Actions from Language and Vision), which is an open-source simulated benchmark designed for learning long-horizon language-conditioned tasks in robotic manipulation. Specifically, CALVIN propose a set of 34 manipulation tasks in 4 different environments (Figure \ref{fig:calvin_env}). 
\input{figures/ch1/calvin.tex}
This benchmark was used by \cite{mees2022hulc,mees2023hulc++,reuss2024multimodal}. Specifically, the work proposed in \cite{reuss2024multimodal} is actually the best performing method on the CALVIN benchmark. The authors proposed an architecture that is able to handle goals described in terms of both natural language description and goal image, moreover they used a Diffusion Transformer Model as policy (Figure \ref{fig:mdt_architecture}). To reach this goal, the authors had to solve first of all the problem related to how to force the Multimodal Transformer Encoder to generate the same behavior indipendently from the goal modality. To solve this problem authors of MDT proposed two auxiliary self-supervised loss functions:
\begin{itemize}
    \item \textit{Masked Generative Foresight} (MGF) is a reconstruction-loss function designed to ensure that the MDT generates embeddings capable of guiding the robot behavior consistently across different modalities. This means that the tokens generated by the MDT can be used to construct image patches representing feature states, whether the goal is specified in terms of an image or a language description. Specifically, given the latent embedding of the MDT encoder for state $s_i$ and goal $g$, MGF trains a Vision Transformer (ViT) to reconstruct a sequence of 2D image patches $(u_1, \dots, u_U) = \text{patch}(s_{i+v})$ corresponding to the future state $s_{i+v}$.
    \item \textit{Contrastive Latent Alignment} (CLA) is a contrastive loss term designed to encourage the MDT to align the embeddings generated from a goal image with those generated from a language description. For each training sample $(s_i, a_i)$ paired with a multimodal goal specification $G_{s_i,a_i} = \{o_i, l_i\}$, CLA reduces the image and language goals to vectors $z_i^o$ and $z_i^l$, respectively. The CLA loss is then computed using the InfoNCE loss, based on the cosine similarity $C(z_i^o, z_i^l)$ between the image-goal conditioned state embedding $z_i^o$ and the language-goal conditioned state embedding $z_i^l$.
\end{itemize}
\input{figures/ch1/mdt_architecture.tex}
As mentioned, this method was tested on the CALVIN benchmark, specifically in the $ABCD \rightarrow D$ test scenario, where the model was trained on the $ABCD$ environments and tested on environment $D$. The CALVIN benchmark comprises a set of 1000 rollouts, with each rollout consisting of a sequence of 5 commands. It is noteworthy that the MDT architecture successfully completed $80\%$ of the rollouts up to the fifth command. In particular, the MGF loss was observed to have a significant impact on system performance, improving the success rate for the fifth command from $69.8\%$ to $79.4\%$. This demonstrates that making the embedding informative about the system's evolution can meaningfully guide the diffusion system, which predicts actions over a certain time horizon into the future.

In the context of Language Conditioned MTIL, other important works to cite include \cite{shridhar2022cliport, shridhar2023perceiver}. Specifically, the authors in \cite{shridhar2022cliport} introduced CLIP-Port, a two-stream architecture that explicitly models the two key tasks in language-conditioned imitation learning: reasoning about \textbf{what to do} and reasoning about \textbf{how to do it}. The former task, referred to as \textit{semantic reasoning}, is derived from the text-based command and is handled using a pre-trained CLIP architecture \cite{radford2021learning}. The latter task, known as \textit{spatial reasoning}, is managed by leveraging the Transporter architecture \cite{zeng2021transporter}. The overall architecture (Figure \ref{fig:clip_port_architecture}) is an Encoder-Decoder framework that ultimately outputs an \textbf{affordance map}, which identifies the locations for executing pick or place operations (Figure \ref{fig:clip_port_affordance}).

During testing, this method was not directly compared with other Language Conditioned MTIL approaches. However, the results obtained allow for several observations. Notably, for tabletop manipulation tasks, the ability to reason using both spatial and semantic features enables a high success rate (ranging from $80\%$ to $90\%$) on tasks involving seen object attributes, such as color, even with a relatively low number of demonstrations (100). However, similar to the results reported in Table \ref{table:rt1_results}, CLIP-Port also struggles with entirely novel tasks, as evidenced by a significant drop in performance when dealing with unseen object attributes and a lower number of demonstrations.

\input{figures/ch1/clip_port.tex}

In conclusion, as discussed in this paragraph, considerable research effort has been dedicated to addressing the problem of Language Conditioned MTIL, particularly in terms of architectural designs and learning strategies. However, it remains challenging to draw definitive conclusions from these various approaches, as they are generally not evaluated on a common benchmark. Despite this issue, some trends can be observed. There is clearly room for improvement in the generalization capabilities of these methods, particularly in handling novel scenarios and tasks. Additionally, data efficiency remains a concern, as there is a noticeable decline in performance as the number of expert trajectories decreases. Another challenge is the ability to manage cluttered scenes with \textbf{relevant distractors}, objects that may have been manipulated during training but must be ignored in the target task, which can lead to occlusion and/or confusion.

\textit{Visual Conditioned}
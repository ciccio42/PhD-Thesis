\paragraph*{Interactive Imitation Learning}\mbox{}\\
The Interactive Imitation Learning approach encompasses all methods specifically designed to address or mitigate the compounding error phenomenon, which was first described in~\cite{pomerleau1988alvinn}.

As previously mentioned, this issue arises because, although Behavioral Cloning primarily follows a supervised learning procedure, it does not satisfy the i.i.d. assumption. The agent's actions influence subsequent observations, creating dependencies between samples in the training set. Consequently, when the agent interacts with the environment, even small errors can lead to new observations outside the training distribution, potentially resulting in an unrecoverable situation that the robot cannot resolve autonomously.

The significance of this problem was first formalized in \cite{ross2010efficient_reductions}. The authors observed that if a system makes an error with probability $\epsilon$ in a task with a time horizon of $T$, then, due to the compounding of errors, a supervised learner incurs a quadratic total cost of $O(\epsilon \ T^{2})$, instead of the expected linear cost of $O(\epsilon \ T)$. The quadratic term arises because, at any given time step $t$, the agent's state is influenced by errors made in the previous $t-1$ steps. This cumulative effect breaks the independence assumption typically held in the i.i.d. setting.

To attenuate this problem, \textbf{interactive supervised learning algorithms} have been proposed, such as the well-known \textit{DAgger} \cite{ross2011dagger}. Algorithm \ref{alg:dagger} describes the DAgger procedure. It is an aggregation strategy, based on the idea to train the policy $\pi^{L}$ under the state-distribution induced by the policy itself, but with the correct action performed by the expert. The main problem with DAgger is that it requires the expert to interact with the system during the training, introducing both \textbf{safety} and \textbf{data-efficiency} problems, especially when the system does not provide the human expert with sufficient control authority during the sampling process \cite{laskey2017comparing_hc_rc}. 
\input{algorithm/dagger.tex}

Human-Guided DAgger (HG-DAgger) \cite{kelly2019hg_dagger} is an enhancement of the traditional DAgger strategy, where a human expert oversees the rollout of the current policy. If the agent moves into an unsafe region of the state space, the expert steps in to guide the system back to safety. Specifically, HG-DAgger was proposed in the context of autonomous vehicle driving, however, in \cite{jang2022bc_z}, it was shown that HG-DAgger is particularly effective in robotic manipulation tasks. The study found that, given the same total number of episodes, a policy trained exclusively on expert demonstrations has a significantly lower success rate than one trained on a dataset that includes both expert demonstrations and expert corrections.

In the context of Interactive Learning for Robot Manipulation, other works of interest include \cite{mandlekar2020human_in_the_loop,chisari2022correct}. 

In \cite{chisari2022correct}, a human expert provides both \textbf{corrective} and \textbf{evaluative} feedback. The former consists in the human that takes control of the robot to adjust the trajectory, the latter consists in a scalar weight $q$, set to 1 if the trajectory is satisfactory, 0 if the trajectory is not satisfactory, $\alpha$ if the trajectory is adjusted by the expert, where $\alpha$ is the ratio between non-corrected and corrected samples. Then a Neural Network was trained by minimizing a weighted version of the maximum-likelihood, $\mathcal{L}(a_{t},s_{t}) = - q \ log(\pi^{L}_{\theta}(a_{t}|s_{t}))$. Real-world experiments show that with a training time of \textbf{41 minutes}, including environmental reset, it was possible to have an agent capable of performing tasks such as picking up a cube or pulling a plug.
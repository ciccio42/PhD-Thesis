\paragraph*{Single-Task Imitation Learning}\mbox{}\\
This paragraph will review the research conducted in the context of \textit{ Single-Task Imitation Learning}. Specifically, within the scope of robotic manipulation problems, the term ``Single-Task" indicates that the learned policy $\pi^{L}$ can perform only the specific task it has been trained on. For example, if the task involves a pick-and-place operation with a fixed place position, the model cannot handle variations in the place location. Additionally, the focus will be primarily on methods that use high-dimensional state representations, such as images, processed by deep architectures to solve the problem.

In this scenario, the scientific literature extends far back in time. One of the seminal works in this field was proposed in 1988 by Pomerleau, who introduced \textit{ALVINN} \cite{pomerleau1988alvinn}. ALVINN is an autonomous vehicle driving system based on a Neural Network that predicts the steering angle from a synthetic camera image input. The network was trained on pairs of (image, steering angle), with the training procedure framed as a supervised classification problem. This was achieved by discretizing the steering angle into 45 units. Pomerleau's work immediately highlighted the issue of \textbf{compounding error}, which arises from the \textbf{covariate shift phenomenon}. This issue occurs because an action $a_{t}$ influences the subsequent state $s_{t+1}$, which becomes the next sample, thereby violating the i.i.d. assumption of Supervised Learning. This results in a test-data distribution that may differ from the training one. This phenomenon has significant consequences on the expected performance of the system and is addressed by methods discussed in the paragraph on \textit{Interactive Imitation Learning}.

Despite the covariate-shift problem, authors in \cite{zhang2018deep_vr_teleoperation} showed that very interesting performance can be obtained in the context of Robot Manipulation, by means of Behavioral Cloning and high quality demonstrations given by teleportation system. In this work, a Convolutional Neural Network was trained to predict the desired linear-velocity, angular-velocity of the end-effector, and the binary gripper state (open/close), given in input the current RGB-D observation of the scene, and the position of three points of the end-effector, during the last 5 time-steps (Figure \ref{fig:deep_bc}). The system was tested on 10 tasks, and the performance are reported in Table \ref{table:deep_vr_teleoperation_results}. The proposed system achieved a high success rate while evaluating all the tasks. The tests were carried out from different initial conditions but still quite similar to those present in the training set (e.g., the initial object positions have been uniformly distributed within the training regime, with random local variations around these positions). The analysis of failure cases showed that the leading cause of errors was the inability to detect critical points in the task execution, such as closing/opening the gripper to pick/place the object or detect the position of the object of interest in order to avoid collision with it.
\input{figures/ch1/deep_visual_bc.tex}
\input{tables/ch1/deep_bc_teleoperation_results.tex}
Generally speaking, when working with these types of systems, there are various aspects and design choices to consider. These include the selection of the architecture (e.g., temporal-dependent or independent), the type of demonstration (human-generated or machine-generated), the quantity of demonstrations, and so on. The authors in \cite{mandlekar2022matters} identify a set of challenges in \textit{Learning from Offline Human Demonstrations} context and propose an extensive study, offering valuable insights for future works.

Specifically, the authors tested three Imitation Learning algorithms and two Offline Reinforcement Learning algorithms in both simulated and real-world manipulation tasks (Figure \ref{fig:what_matters_task}). The study primarily focused on analyzing the following aspects:

\begin{enumerate}
    \item \textit{Source of Demonstrations}. Comparing system performance with demonstrations generated by hand-written policies (MG), proficient human demonstrators (PH), and multiple human demonstrators with varying levels of expertise in teleoperation (MH).
    \item \textit{Observation Space}. Comparing the performance of methods based on the type of input, whether low-dimensional (gripper pose, gripper fingers position, object position) or image-based (visual observation, gripper pose, gripper fingers position).
\end{enumerate}
\input{figures/ch1/what_matters.tex}

Specifically, by extrapolating the most important results from this study (Table \ref{table:what_matters_res_low_dimensional}), it can be observed that the most promising architecture is the recurrent BC-RNN, particularly for datasets composed of multi-human demonstrations (MH). This architecture outperforms even state-of-the-art offline reinforcement learning algorithms, which tend to struggle with datasets containing trajectories of varying quality.

In conclusion, for real-world tasks, the BC-RNN architecture achieved a success rate of \textbf{96.7}\% on the lift task, \textbf{73.3}\% on the can task, and \textbf{3.3}\% on the tool-hang task. These results are noteworthy, as they essentially demonstrate the feasibility of training a proficient system from offline human demonstrations. However, several aspects must be considered, such as the simplicity of the test scenarios, which include only one object without any distractors, and the potential benefit of incorporating contact information for contact-rich manipulation tasks like the tool-hang.

\input{tables/ch1/what_matters_res.tex}
After this work, further research in the field of Single Task Imitation Learning, focused on exploring novel architecture \cite{shafiullah2022behavior} and learning paradigm \cite{nair2022r3m,cheng2023diffusion,shi2023waypoint}. 

The contribution presented in \cite{nair2022r3m} is particularly noteworthy in the context of the learning paradigm. The authors introduced \textit{R3M} (Reusable Representations for Robotic Manipulation), exploring the use of pre-trained visual backbones for robotic manipulation tasks. In traditional computer vision tasks, such as object detection, backbones pre-trained on large general datasets are commonly fine-tuned for specific problems, greatly reducing training time. However, this approach is less prevalent in robotic manipulation, primarily due to the wide variation in tasks, robot embodiments, and environments. This raises the question of whether fine-tuning can be effectively applied to robotic manipulation challenges.

To address this question, the authors in \cite{nair2022r3m} started with the Ego4D dataset \cite{grauman2022ego4d}, which contains over 3,500 hours of video footage of people engaging in a wide range of tasks, from cooking to socializing to assembling objects, across more than 70 locations worldwide. They trained a ResNet50 \cite{resnet} to generate a robust representation that could be leveraged in robotic manipulation tasks. The authors proposed a self-supervised learning procedure designed to capture the following aspects:

\begin{itemize}
    \item \textit{Temporal Dynamics}. The learned representation should account for features related to physical interactions. To achieve this, the authors introduced a \textbf{Time Contrastive Loss}, implemented as an InfoNCE loss, which creates similar embeddings for time-adjacent frames while keeping frames that are far apart in time or from different videos distinct.
    \item \textit{Semantic Meaning}. The learned representation should encode information related to the task itself. To capture this, the authors introduced a language prediction module. Given the representation for the first frames of a task $k$ ($\phi^{k}_{0}$), the representation for the $i^{th}$ frame of the same or a different task ($\phi^{j}_{i}$), and a language description $l$ referring to task $k$, the system must output a score indicating whether the transition from $\phi^{k}_{0}$ to $\phi^{j}_{i}$ corresponds to the description $l$. In this way, the system is trained to encode a representation that contains semantic features related to the task itself, enabling it to determine whether a given representation corresponds to a specific task.
\end{itemize}

In testing, the authors demonstrated that using the pre-trained R3M representation improves the overall success rate while requiring fewer demonstrations. On the Meta-World benchmark, with just 5 demonstrations, R3M achieved a success rate of nearly \textbf{$60\%$}, compared to \textbf{$30\%$} for a system trained from scratch.


In \cite{cheng2023diffusion}, the authors introduced the ``Diffusion Learning" paradigm for policy learning in robotic manipulation. The core idea behind Diffusion Learning is to model the network output as a \textit{denoising process}. Starting with $x^k$, sampled from Gaussian noise, the denoising process iterates $K$ times, progressively reducing noise to create intermediate samples, \\ $x_k, x_{k-1}, \dots, x_0$, until the noise-free output $x_0$ is achieved. This process is described by the equation in Formula \ref{eq:denoising}, where $\epsilon_{\theta}$ is the noise prediction network, and its parameters are trained during the learning process.

\begin{equation}
    \label{eq:denoising}
    x^{k-1} = \alpha \left( x^k - \gamma \epsilon_{\theta}(x^k, k) + \mathcal{N}(0, \sigma^{2}I)\right)
\end{equation}

The authors adapted this concept to robot manipulation by observing that the denoising process can be applied to an action $a_{t}^{k}$. However, a challenge arises because the model must generate an action conditioned on the current observation. This requires learning the conditional probability distribution $p(a_t \mid o_t)$ instead of the joint distribution $p(a_t, o_t)$. To address this, the authors proposed the architecture shown in Figure \ref{fig:diffusion_model}.
\input{figures/ch1/diffusion_il.tex}

Specifically, in both architectures, the model takes the latest $T_o$ steps of observation data $O_t$ (observation horizon) as input and predicts $T_p$ steps of actions (prediction horizon), of which $T_a$ steps of actions are executed on the robot without re-planning.

After training the system by minimizing the Mean Squared Error Loss, promising results were observed during testing. Specifically, the model was tested on the tasks shown in Figure \ref{fig:what_matters_task} and achieved near-perfect performance on the Lift, Can, Square, Transport, and Tool Hang tasks, demonstrating the diffusion model ability to handle noisy demonstrations from the MH set. Additionally, the Transformer-based architecture achieved an average success rate of $100\%$ on the two most complex tasks in the benchmark, Transport and Tool Hang.

Another significant work in Single-Task Imitation Learning is presented in \cite{shi2023waypoint}. The authors address the issue of compounding errors by proposing a novel approach, distinct from the Interactive Learning Paradigm (discussed in the following paragraph). They observe that a trajectory can be decomposed into a limited number of key states or waypoints. By linearly interpolating between these waypoints, a demonstrated trajectory can be reconstructed with a certain degree of accuracy, reducing the impact of compounding errors. This is because the network predicts far fewer future states compared to methods that predict every step of the trajectory. Based on this insight, the authors introduced the ``Automatic Waypoint Extraction'' (AWE) system. This pre-processing tool takes a trajectory $\tau$ as input and decomposes it into a set of waypoints, from which an approximate trajectory $\hat{\tau}$ can be interpolated with a controlled error margin.

Specifically, to decompose the original trajectory $\tau$ authors formalized the optimization problem in Formula \ref{eq:awe}, where they basically wants to find the minimum number of waypoints $W$ such that a given reconstruction loss $\mathcal{L}$ is lower than a certain margin $\eta$.
\begin{equation}
    \label{eq:awe}
    \min_{W} |W|  \ s.t. \mathcal{L}((f(W), \tau)) \le \eta
\end{equation}
They solved this problem by implementing a Dynamic Programming based algorithm, which identifies the shortest subsequence such that the reconstruction loss is less than $\eta$, while ensuring that the points in the subsequence are restricted to the original trajectory. 

The authors applied this preprocessing tool in conjunction with state-of-the-art architectures, such as the Diffusion Policy \cite{cheng2023diffusion}, to the same tasks presented in \cite{mandlekar2022matters}. Notably, they observed that, compared to the near-optimal results of \cite{cheng2023diffusion}, the Diffusion Policy combined with the AWE system achieves good performance with significantly less data. Specifically, in the Square task, the system achieved an average success rate of $91.7\%$ with only 100 demonstrations, compared to $82.0\%$ with the Diffusion Policy alone.

In conclusion, there is significant research interest in the field of Imitation Learning for robotic manipulation tasks. However, the primary limitation of these Single-Task methods is that they fall short of the ideal concept of a \textbf{general-purpose robot} capable of solving any prompted task, which is the focus of this thesis. To address this limitation, \textbf{Multi-Task Imitation Learning} systems have been proposed. Indeed, as stated in Section \ref{sec:motivation}, one goal is to have an adaptable and general system able to perform multiple-prompted task. These methods will be discussed in details in the Section \ref{sec:occp_related_works} of the core Chapter \ref{ch:occp}. Despite this, the discussion of these methods is crucial, as the approaches presented later in this thesis build upon the foundational concepts introduced here.

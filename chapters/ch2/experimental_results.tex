\section{Experiments}
\label{sec:cod_experimental}
In this section, the performed experiments are going to be described. Specifically, in  
Section~\ref{sec:cod_dataset} the dataset used for training procedure will be described. Section~\ref{sec:cod_results} will report the obtained results.
\subsection{Dataset}
\label{sec:cod_dataset}
To validate the proposed architecture, a simulated dataset was generated, focusing on four primary tasks: \textit{Pick-Place}, \textit{Nut-Assembly}, \textit{Stack-Block}, and \textit{Press-Button}. Each task consists of multiple variations. Specifically, the Pick-Place task has 16 variations, Nut-Assembly has 9 variations, Stack-Block has 6 variations, and Press-Button has 6 variations. Each task and its variations are graphically described in Figure~\ref{fig:dataset_cod}.
\input{figures/ch2/dataset_cod.tex}

For each task, 100 trajectories were collected for both the agent (UR5e robot) and the demonstrator (Franka-Emika Panda robot) using hand-written policies that take as input ground-truth information about the object position. Across these trajectories, the position of the objects of interest varies. The workspace for each task is divided as follows:
\begin{itemize}
    \item \textbf{Pick-Place}: The bins are fixed in position, while the boxes can change according to the following rule. The workspace is divided into 4 slots, parrallel to the bins, each with a width of $6 \ \text{cm}$ and a height of $15 \ \text{cm}$. A slot is randomly selected, and the box is randomly spawned within the chosen slot, ensuring that each slot is filled with only one object.
    \item \textbf{Nut-Assembly}: The pegs are fixed in position, while the nuts vary in placement. A spawn region of $75 \times 10 \ \text{cm}$ is defined, and the nuts are randomly spawned in this region, ensuring they do not collide with one another.
    \item \textbf{Stack-Block}: The placing box is spawned in a region of $16 \times 5 \ \text{cm}$, while the target box is spawned in a region of $24 \times 5 \ \text{cm}$, ensuring no collisions between the boxes.
    \item \textbf{Press-Button}: The two sets of buttons are placed within a region of $4 \times 4 \ \text{cm}$.
\end{itemize}

The bounding boxes are generated automatically using a procedure that assumes prior knowledge of the object's pose and dimensions, as well as the camera's pose and intrinsic parameters. With this information, a 3D bounding box can be constructed around the object in continuous world space. The corners of this bounding box are then projected into the 2D discrete camera space via a sequence of transformations, as described in Equation \ref{eq:transformation_from_world_to_px}.

\begin{equation}
    \label{eq:transformation_from_world_to_px}
    \begin{matrix}
        \tilde{p}^{camera}_{object} = T^{camera}_{world} \times \tilde{p}^{world}_{object}
        \\ \\
        px_x =  \frac{p_x}{p_z} \cdot f + \frac{w}{2} 
        \\ \\
        px_y =  \frac{p_y}{p_z} \cdot f + \frac{h}{2}
    \end{matrix}
\end{equation}

In this equation, $T^{camera}_{world} \in \mathcal{R}^{4 \times 4}$ represents the homogeneous transformation matrix that defines the camera orientation and position relative to the world frame. The term $\tilde{p}^{src}_{target} = \left[ p_x, p_y, p_z, 1 \right]$ denotes the homogeneous position vector of the \textit{target} frame relative to a given \textit{src} frame. For instance, $\tilde{p}^{world}_{object}$ describes the position of the object relative to the world frame. 

The parameters $w$ and $h$ correspond to the image resolution width and height, respectively. The camera's focal length $f$ is given by $f = \frac{0.5 \times h}{\tan(fov_y)}$, where $fov_y$ is the vertical field of view. Finally, $px_x$ and $px_y$ represent the pixel coordinates obtained in the x and y axes, respectively.

% \smalltodo{add equation}
\subsection{Results}
\label{sec:cod_results}
This section presents the obtained results, divided into two main blocks. The first block (Section~\ref{sec:cod_tod}) discusses the results of the method trained to detect only the target object. The second block (Section~\ref{sec:cod_tod}) covers the results of the method trained to detect both the target object and the final (placing) location. For each method, results are reported for two different scenarios: first, where the method is trained in a single-task multi-variation scenario; and second, where the method is trained in a multi-task multi-variation scenario.

For all the test configurations described below, the testing procedure was consistent. Specifically, for each task and its variations, 10 rollouts were performed. During each rollout, the robot was controlled by a hand-written policy, and the predicted bounding box was compared with the ground truth, obtained using the same procedure as described earlier.

The metrics used to evaluate the methods were \textit{Precision@0.5} (Equation \ref{eq:prec}) and \textit{Recall@0.5} (Equation \ref{eq:rec}).
\begin{equation}
    \label{eq:prec}
    Pre = \frac{TP}{TP+FP}
\end{equation} 
\begin{equation}
    \label{eq:rec}
    Rec = \frac{TP}{TP+FN}
\end{equation}

A \textit{True Positive} (TP) sample is defined as a predicted bounding box for the target class with an Intersection over Union (IoU) greater than or equal to 0.5 when compared with the ground-truth bounding box. A \textit{False Positive} (FP) sample is defined as a predicted bounding box for the target class with an IoU less than 0.5. A \textit{False Negative} (FN) sample is defined as a target object for which no bounding box was predicted.

The metrics were computed considering only the target bounding boxes, i.e., the bounding boxes predicted for the target object or the target location. Among all the candidates generated by the network, only the one with the highest predicted class score was considered.


\subsubsection{Target object detector}
This first set of validation tests is related to the training of the COD module in a setting where it must predict the location of the target-object. This means that, with respect to the semantic attributes defined in Section \ref{sec:cod_problem}, the set is restricted to just two classes ``target'' and ``no-target''. In this case, the method will be named \textit{Conditioned Target Object Detector} (CTOD).
\label{sec:cod_tod}
\paragraph*{Single-task multi-variation scenario}\mbox{}\\
In the single-task multi-variation scenario, a specific model was trained for each task described in Figure \ref{fig:dataset_cod}. This approach allows for evaluating the model ability to handle tasks of increasing complexity. Initially, the model is evaluated in a simpler scenario, where it predicts the target position of a specific category of objects (e.g., only boxes in pick-and-place, or only nuts in nut-assembly), thereby limiting variability in object categories.
\input{tables/ch2/ctod_performance.tex}
Table \ref{table:ctod_single_task_performance} presents the overall performance of the CTOD module in terms of Precision and Recall, providing a comprehensive evaluation of the system ability to accurately identify target objects (Precision) and consistently detect them across multiple rollouts (Recall). The results demonstrate that the module successfully identifies target objects with precise bounding boxes and maintains a high level of consistency across rollouts, achieving excellent precision and recall metrics. 

In particular, the Recall remains consistently at \textbf{1.00}, indicating the absence of false negatives, meaning the system always generates a bounding box classified as the ``target" object. Precision values are also high, exceeding \textbf{0.90} for three tasks: Nut-Assembly, Stack-Block, and Press-Button. However, a lower Precision is observed for the Pick-Place task. This is due to the task longer motion duration, both in terms of time and space, which introduces varying scales of the target object throughout the motion, increasing the complexity of predicting accurate bounding boxes. Specifically, out of the 2612 false positives generated for the Pick-Place task, \textbf{2125} were produced after the picking phase, while only \textbf{487} occurred during the reaching phase.

Generally, the obtained performance demonstrate that the module not only generates bounding boxes with the correct ``target" classification but also produces highly accurate bounding boxes that closely match the ground truth. Figure \ref{fig:ctod_example_of_prediction} shows examples of predictions for all tasks during the execution of a rollout. As observed, during the approaching phase, crucial for avoiding target misidentification, the bounding box remains accurately positioned around the target object. However, during the robot motion, the bounding box slightly shifts, occasionally resulting in false positives.

\input{figures/ch2/ctod_example_of_predictions.tex}

\paragraph*{Multi-task multi-variation scenario}\mbox{}\\
In the multi-task multi-variation scenario, a single model was trained to perform all four tasks. This setup enables a more rigorous validation of the COD module in a complex environment, where objects of different shapes and sizes are involved across various tasks. Table \ref{table:ctod_multi_task_performance} presents the overall performance of the COD module in terms of Precision and Recall. In this scenario, the system consistently generates bounding boxes for the "target" class (achieving a Recall of 1.00 for all tasks) with an average overlap greater than 0.5. A similar trend as in the single-task scenario is observed, with a lower Precision for the Pick-Place task due to the same reasons previously explained.

Furthermore, compared to the single-task setting, a slight reduction in Average IoU is observed, which can be attributed to the increased complexity of the task. In this multi-task scenario, a single Fast R-CNN must predict bounding boxes across a diverse set of manipulation tasks, where objects may be similar but appear at different scales in the input images. This variation adds further complexity to the prediction process.


\subsubsection{Target object and final position detector}
\label{sec:cod_tofpd}
This set of validation tests focuses on training the COD module in the complete scenario described in Section \ref{sec:cod_problem}. In this setting, the COD module is trained to predict bounding boxes for both the target object and the final location of interest. Specifically, for the three tasks involving the ``pick-and-place" primitive (i.e., Pick-Place, Nut-Assembly, and Stack-Block), the final location corresponds to the target bin, peg, and block to be stacked, respectively. In contrast, for the Press-Button task, the target location is defined as the bounding box of the initial button, translated to the final position of the pressed button (Figure \ref{fig:press_button_target_placing}).
\input{figures/ch2/press_button_target_placing.tex}

\input{tables/ch2/cod_performance.tex}
\paragraph*{Single-task multi-variation scenario}\mbox{}\\
Table \ref{table:cod_single_task_performance} summarizes the performance of the COD module in the single-task, multi-variation evaluation setting. It can be observed that, even in this scenario, the COD module consistently identifies both the target object and the target placement location. Specifically, the average Intersection over Union ($IoU_{avg}$) is generally higher for the target-placement class. This is because, for the Pick-Place, Nut-Assembly, and Stack-Block tasks, the placement location is fixed, making the detection problem easier for the conditioning module to address. 

Regarding the ``target" class, the $IoU_{avg}$ is lower compared to Table \ref{table:ctod_single_task_performance}, with a significant drop observed in the Pick-Place task. However, by examining the distribution of false positives, it can be noted that, out of \textbf{7565} false-positive cases, \textbf{2001} occurred during the reaching phase, while the remaining \textbf{5564} occurred during the placing phase. 

It is worth noting that the module can still identify the region of the target object. In fact, by lowering the IoU threshold to 0.1, the number of false positives during the reaching phase drops to 155, indicating that the system is able to detect the region of interest, albeit with lower precision. Nevertheless, for the task at hand, having consistently precise bounding boxes is not critical, as the control module is designed to be robust against minor errors or imprecision in bounding box predictions.


\paragraph*{Multi-task multi-variation scenario}\mbox{}\\
The final evaluation setting essentially confirms the trends observed in the previous one. Table \ref{table:cod_multi_task_performance} summarizes the results obtained from the COD module trained in the multi-task scenario. In this case, a slight drop in Precision is again observed, which corresponds to a decrease in the average Intersection over Union ($IoU_{avg}$). This decline can be attributed to the same reasons discussed in the previous multi-task setting, with the added complexity that the module must also predict the bounding box for the target placement location.


In conclusion, this paragraph has introduced the COD module, a Conditioned-Convolutional Neural Network designed to address a novel object-detection problem. The module predicts category-agnostic bounding boxes for the two key regions in a manipulation task: the region where the object to be manipulated is located and the final region where the task is completed (e.g., placing the box, assembling the nut, stacking the block, or pressing the button). In a highly generalized multi-variation scenario, where the object position is not predefined, and the semantic attribute assigned to the object is dynamically specified by a command, which is given through a video demonstration of another agent performing the desired task.


The results demonstrate that the proposed module effectively handles both single-task and multi-task scenarios, generating bounding boxes that are correctly classified and accurately identify the regions of interest.

In Chapter \ref{ch:occp}, it will be shown how the positional information from this command can be effectively utilized to inform the control module about the regions of interest, addressing the problem of target misidentification.

\chapter{Conclusions}

In conclusion, this thesis is framed in the context of \textit{Learning from Demonstration}, a supervised learning problem that leverages data-driven methods to learn control functions, enabling robots to perform tasks. Section \ref{sec:sota} explores the formulation of the learning problem, addressing all its aspects, from the mathematical foundations to the techniques used for data collection (Section \ref{sec:problem_formulation}), and concludes with a comprehensive taxonomy of how this problem has been addressed in the literature (Section \ref{sec:lfd}). From this analysis, it is clear that the current methodologies do not lead to a ``general-purpose" robotic platform. Instead, the learned policies are typically limited to controlling the robot for the specific task on which they were trained, whith limited generalization capabilities to different initial configurations of known objects. This limitation prevents a human operator from commanding the robot to perform arbitrary tasks, a highly desirable feature in agile and user-friendly robotic platforms, especially in modern industrial collaborative environments.

For this reason, this thesis addresses the problem of \textit{Multi-Task Imitation Learning} (Section \ref{sec:occp_mtil}), where a policy is learned to perform multiple variations of a given task, as well as multiple distinct tasks. The policy also incorporates the capability to select, at runtime, the desired task to execute, either through commands based on textual prompts or by using videos of other agents performing the desired task in different configurations. Among these two modalities, the thesis focuses on \textit{Video-Conditioned Multi-Task Imitation Learning}, which aims to train control policies where the desired task is specified via video demonstrations of other agents completing the task.

Specifically, in this thesis a different approach to the problem is leveraged. Indeed, started from the consideration that in the Multi-Task scenario there are two main problem to be solved, one related to the \textbf{command analysis and understanding}, which can be seen as a \textit{cognitive-problem}, where starting from the current observation of the scene and the command, the system must be able to understand the task intent by lokalizing for example the objects of interest, and the second related to the \textbf{action generation}, which can be seen as a \textit{control-problem}, where starting from the current agent state and the information coming from the command analysis, must generate the correct action towards the task completion. Based on these considerations, an intuitive way to model the problem is through a modular system, however all the state-of-the-art methods proposed in literature for the problem in hand are based on end-to-end architectures (Section \ref{sec:occp_related_works}), which means that models starting from high-dimentional inputs (images), are converted into low-level actions, generally the pose of the robot's end-effector with respect to some fixed reference frame. 

In the state-of-the-art methods validation (Section \ref{sec:ocpl_results}) it was observed how the SoTA end-to-end architectures are able to generate valid and reasonable trajectories, however they complete the task by manipulating the wrong object, this means that the Backbone Module is not able to inform the Control Module with the position of the target object. Indeed, it was observed that by performing just 2 ground truth actions (e.g., actions generated trough an hand-written control with access to ground state information), the success-rate improved in a considerable way.

For this reason the problem of \textit{Conditioned Object Detection} has been formulated and proposed in Chapter \ref{ch:cod}, where a conditioned convolutional neural network has been proposed with the ability to generate category agnostic bounding-boxes that given in input the current agent observation and the command is able to localize the objects of interest (e.g., the box to manipulate as well as the bin where place the object). This module was experimental tested in a simulated environment both in a Single-Task Multi-Variation and in a Multi-Task Multi-Variation, and it was observed how the proposed module is actually able to identify the locations of the objects of interest with an high precision, considering that the lowest average IoU is equal to \textbf{0.563}. 

Once the COD module was validated and tested, it was integrated into the SoTA framework to validate the hypothesis that solving the localization problem with a dedicated module can reduce the \textbf{object misidentification problem}, thereby improving both the overall success rate and the system's interpretability. The generated bounding box provides information about where the robot is going to move. Specifically, two modules were proposed and tested: MOSAIC-CTOD and MOSAIC-COD. The first module integrates the CTOD module (Section \ref{sec:cod_tod}), which detects only the target object, while the second module integrates the COD module (Section \ref{sec:cod_tofpd}), which detects both the target object and the final location of interest.

These modules were tested in both single-task multi-variation scenarios and multi-task multi-variation scenarios. The results, described in Section \ref{sec:ocpl_results_scm} and Section \ref{sec:ocpl_results_dcm}, show that in the single-task setting, the target object prior effectively facilitates consistent and robust target object reaching and picking, with a picking rate always greater than \textbf{80\%}. 

Regarding the success rate, the highest average value was achieved with the final MOSAIC-COD module, which across four tested tasks showed an average success rate of \textbf{90.13\%}. This highlights how including information about the object's location improves system robustness. 

In the multi-task scenario, it was observed that the use of object priors also resulted in a more successful and robust system. The MOSAIC-COD system achieved an average success rate of \textbf{79.24\%}, a significant improvement compared to the baseline (46.01\%). However, there is still room for improvement, particularly in balancing performance across different tasks.

In conclusion, the proposed system was also validated in a real-world scenario (Chapter \ref{ch:real_world_application}). Specifically, MOSAIC-CTOD and MOSAIC-COD were tested in a restricted single-task, multi-variation scenario with two different configurations. The first configuration involved training the system from scratch, while the second used a system finetuned from a model pre-trained in simulation. Results show that the finetuned systems outperformed the systems trained from scratch, demonstrating the potential of leveraging different data domains to initialize the system, which can then be finetuned for the target context.

It was generally observed that the C(T)OD module could predict the location of target objects based on demonstrations from a simulated agent, indicating that the system can interpret task execution from demonstrations by different agents and in different domains. This opens interesting possibilities for future development, including the use of human-based demonstrations.

Regarding the final control policy, the highest success rate was achieved with the MOSAIC-COD module, reaching \textbf{55.00\%}. While this success rate is lower compared to the corresponding results in simulation, several factors need to be considered, including the characteristics of the dataset and the real-world challenges. Notably, the system consistently reached the target object with a reaching rate of \textbf{86.67\%}, indicating that the system can reliably inform the control module about the target object's position. The primary source of errors involved collisions with the target object, mostly due to noisy and low-quality data. These issues could be mitigated by enhancing the robot's perceptual capabilities, such as using depth information or a camera mounted on the robot.

\chapter{Conclusions}
\label{sec:conclusion}
In conclusion, this thesis is framed in the context of \textit{Learning from Demonstration}, a supervised learning problem that leverages data-driven methods to learn control functions, enabling robots to perform tasks. Section \ref{sec:sota} explores the formulation of the learning problem, addressing all its aspects, from the mathematical foundations to the techniques used for data collection (Section \ref{sec:problem_formulation}), and concludes with a comprehensive taxonomy of how this problem has been addressed in the literature (Section \ref{sec:lfd}). From this analysis, it is clear that the current methodologies do not lead to a ``general-purpose" robotic platform. Instead, the learned policies are typically limited to controlling the robot for the specific task on which they were trained, with limited generalization capabilities to different initial configurations of known objects. This limitation prevents a human operator from commanding the robot to perform arbitrary tasks, a highly desirable feature in agile and user-friendly robotic platforms, especially in modern industrial collaborative environments.

For this reason, this thesis addresses the problem of \textit{Multi-Task Imitation Learning} (Section \ref{sec:occp_mtil}), where a policy is learned to perform multiple variations of a given task, as well as multiple distinct tasks. The policy also incorporates the capability to select, at runtime, the desired task to execute, either through commands based on textual prompts or by using videos of other agents performing the desired task in different configurations. Among these two modalities, the thesis focuses on \textit{Video-Conditioned Multi-Task Imitation Learning}, which aims to train control policies where the desired task is specified via video demonstrations of other agents completing the task.

Specifically, in this thesis a different approach to the problem is leveraged. Indeed, started from the consideration that in the Multi-Task scenario there are two main problems to be solved, one related to the \textbf{command analysis and understanding}, which can be seen as a \textit{cognitive-problem}, where starting from the current observation of the scene and the command, the system must be able to understand the task intent by localizing for example the objects of interest, and the second related to the \textbf{action generation}, which can be seen as a \textit{control-problem}, where starting from the current agent state and the information coming from the command analysis, must generate the correct action towards the task completion. Based on these considerations, an intuitive way to model the problem is through a modular system, however all the state-of-the-art methods proposed in literature for the problem in hand are based on end-to-end architectures (Section \ref{sec:occp_related_works}), which means that models starting from high-dimentional inputs (images), are converted into low-level actions, generally the pose of the robot's end-effector with respect to some fixed reference frame. 

In the state-of-the-art methods validation (Section \ref{sec:ocpl_results}) it was observed how the SoTA end-to-end architectures are able to generate valid and reasonable trajectories, however they complete the task by manipulating the wrong object, this means that the Backbone Module is not able to inform the Control Module with the position of the target object. Indeed, it was observed that by performing just 2 ground truth actions (e.g., actions generated through an hand-written control with access to ground state information), the success-rate improved in a considerable way.

For this reason the problem of \textit{Conditioned Object Detection} has been formulated and proposed in Chapter \ref{ch:cod}, where a conditioned convolutional neural network has been proposed with the ability to generate category agnostic bounding-boxes that given in input the current agent observation and the command is able to localize the objects of interest (e.g., the box to manipulate as well as the bin where place the object). This module was experimental tested in a simulated environment both in a Single-Task Multi-Variation and in a Multi-Task Multi-Variation, and it was observed how the proposed module is actually able to identify the locations of the objects of interest with a high precision, considering that the lowest average IoU is equal to \textbf{0.563}. 

Once the COD module was validated and tested, it was integrated into the SoTA framework to validate the hypothesis that solving the localization problem with a dedicated module can reduce the \textbf{object misidentification problem}, thereby improving both the overall success rate and the system's interpretability. The generated bounding box provides information about where the robot is going to move. Specifically, two modules were proposed and tested: MOSAIC-CTOD and MOSAIC-COD. The first module integrates the CTOD module (Section \ref{sec:cod_tod}), which detects only the target object, while the second module integrates the COD module (Section \ref{sec:cod_tofpd}), which detects both the target object and the final location of interest.

These modules were tested in both single-task multi-variation scenarios and multi-task multi-variation scenarios. The results, described in Section \ref{sec:ocpl_results_scm} and Section \ref{sec:ocpl_results_dcm}, show that in the single-task setting, the target object prior effectively facilitates consistent and robust target object reaching and picking, with a picking rate always greater than \textbf{80\%}. 

Regarding the success rate, the highest average value was achieved with the final MOSAIC-COD module, which across four tested tasks showed an average success rate of \textbf{90.13\%}. This highlights how including information about the object's location improves system robustness. 

In the multi-task scenario, it was observed that the use of object priors also resulted in a more successful and robust system. The MOSAIC-COD system achieved an average success rate of \textbf{79.24\%}, a significant improvement compared to the baseline (46.01\%). However, there is still room for improvement, particularly in balancing performance across different tasks.

In conclusion, the proposed system was also validated in a real-world scenario (Chapter \ref{ch:real_world_application}). Specifically, MOSAIC-CTOD and MOSAIC-COD were tested in a restricted single-task, multi-variation scenario with two different configurations. The first configuration involved training the system from scratch, while the second used a system finetuned from a model pre-trained in simulation. Results show that the finetuned systems outperformed the systems trained from scratch, demonstrating the potential of leveraging different data domains to initialize the system, which can then be finetuned for the target context.

It was generally observed that the C(T)OD module could predict the location of target objects based on demonstrations from a simulated agent, indicating that the system can interpret task execution from demonstrations by different agents and in different domains. This opens interesting possibilities for future development, including the use of human-based demonstrations.

Regarding the final control policy, the highest success rate was achieved with the MOSAIC-COD module, reaching \textbf{55.00\%}. While this success rate is lower compared to the corresponding results in simulation, several factors need to be considered, including the characteristics of the dataset and the real-world challenges. Notably, the system consistently reached the target object with a reaching rate of \textbf{86.67\%}, indicating that the COD can reliably inform the control module about the target object's position. The primary source of errors involved collisions with the target object, mostly due to noisy and low-quality data. These issues could be mitigated by enhancing the robot's perceptual capabilities, such as using depth information or a camera mounted on the robot.

% Implications of the findings:
% 1. Sistema che è in grado di replicare un task eseguito da un altro agente in un dominio diverso, aprendo quindi la possibilità di avere sistemi robotici in grado di imparare l'esecuzioni di nuovi task a partire da dimostrazioni video di altri agenti, anche in domini diversi.
% 2. Questo attraverso un approccio modulare, dove 
\smalltodo{Commento  Benoit: the conclusions could include more detailed perspectives on future work and broader implications of the findings.}
\smalltodo{Commento  George: the discussion could benefit from a deeper exploration of potential limitations and future research directions, such as scalability to more complex environments or broader task sets.}

\updated{
In conclusion, the methods discussed in this thesis represent a significant advancement toward developing a system capable of replicating tasks performed by another agent, even under varying environmental conditions (e.g., differences in object positions between the demonstrator and the agent) and in challenging scenarios involving multiple similar objects, where the semantic role of each object (e.g., target or distractor) is dynamically defined at runtime by the demonstrator. The proposed approach, based on a modular architectures, has demonstrated its effectiveness. Specifically, by first localizing the target objects, not only is the control problem simplified, since the network learns a simpler mapping from pixel coordinates to spatial coordinates, but also the interpretability of the system is enhanced. The predicted bounding box serves as \textit{``human-readable"} information, which can be used to estimate where the robot is going to move.

Generally speaking, such capabilities are highly desirable when the goal is to develop general-purpose robotic platforms that are capable not only of performing multiple tasks but also of learning to execute novel tasks through an \textit{intuitive programming paradigm} based on \textit{video demonstrations}. For instance, in a highly adaptable manufacturing scenario, a human operator could demonstrate a new assembly procedure by recording just a few executions. The robot could then be tasked with learning the procedure and replicating it across various configurations.

Building on the proposed modular approach, future works could focus on the following aspects:
\begin{itemize}
    \item \textbf{Scalability to more complex objects}. Once the modular approach has demonstrated its effectiveness with simple-shaped objects (e.g., boxes and bins), the system can be enhanced to handle more complex objects, such as those with irregular shapes or multiple parts that need to be assembled. To achieve this, more advanced \textit{conditioned object detection} techniques can be employed, drawing inspiration from novel Visual-Question Answering models \cite{wen2024object}, where Visual-Language Architectures are used to selectively localize the objects of interest based on a given query. 

    \item \textbf{Generalization to multi-step tasks}. In industrial settings, tasks are often composed of multiple steps, requiring the agent to perform a sequence of actions to complete it. To address this challenge, a novel methodological approach is needed to decompose the task into sub-tasks. One possible approach is to use a Graph-Based scene representation, where each node corresponds to an object and the edges represent the relationships between them. This representation can then be used to model the current task state and facilitate learning a policy that generates the actions needed to transition from one state to another, i.e., determining how the current graph must be modified to achieve the desired goal of the $i$-th sub-task. 
\end{itemize}
}


% This kind of ability is achieved by leveraging a \textit{\textbf{modular approach}}, where first the target objects are localized within the agent space, and then the control policy generates the actions needed to manipulate the object and solve the demonstrated tasks. Improving not only the success-rate, as extensively demonstrated in the results, but also the interpretability of the system, since the predicted bounding-box can be seen as a \textit{``human-readable"} information, which can be used to estimate where the robot is going to move.  

% Overall, 
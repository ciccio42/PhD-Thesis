\chapter*{Abstract}
% 5 PAGINE E UN PO
\label{ch:abstract}
Robot technology is one of the pillars of modern society. Advances in information, electronic, and mechanical fields enable us to build and program machines to perform tasks in very different contexts, such as industry, surgery, and space missions.
\newline Specifically, in manufacturing, robots are mainly used to perform repetitive and unhealthy works like assembly, welding and material handling, thanks to their mechanical robustness and ability to repeatedly perform the same movements with high accuracy and precision.

While in the early day, robot systems were constrained in isolated and known environments. Over the past few decades, robots have been asked to solve tasks in dynamic and unknown/partially known environments, where they must \textbf{coexist} and \textbf{cooperate} with humans, while  solving different \textbf{dynamic} tasks \cite{bini2023multi} (e.g. pick a requested object, whose position is not known a priori). 

In this scenario, the desired characteristics of such robotic systems are:
\begin{enumerate}[label=\textbf{(\alph*)}]
\item \textbf{Adaptability to new conditions}, i.e., the system must be able to easily adapt to dynamic changes in system and environmental conditions, performing \textit{``intelligent" behaviors} to handle these new scenarios and solve the desired task.
\item \textbf{Adaptability to new tasks}, i.e., the system must be able to easily adapt to both new variations of a known task and completely new tasks by exploiting experience to infer actions and solve them.
% \item \textbf{Reliability}, i.e., the proposed system must be reliable in terms of success rate (tasks must be solved with a success rate that is reasonable for a real-world application), and safety (actions must not harm the environment, people, or the robot itself);
% \item \textbf{Computational efficiency}, i.e., the system must meet all the constraints above and run in real-time on real-world robot platforms as efficiently as possible.
\end{enumerate}

These requirements, can be challenging to achieve with traditional robot programming techniques based on hand-written policies and control methods. These conventional techniques often require a meticulous analysis of process dynamics, the construction of an analytical model, and the derivation of a control law that meets specific design criteria. This design process is tedious and time-consuming, particularly when high-level perception systems (e.g., cameras, microphones, motion sensors) are used to infer the state of the environment (such as the unknown position of a desired object relative to the end-effector) and the intentions of the human operator.

% In questo caso la funzione di controllo viene appresa dai dati, nello specifico, nel primo caso 
In contrast, significant advancements have been made by leveraging \textit{learning techniques}, where the control policy is inferred from data. This data can be generated either by \textbf{agent experience} \cite{sutton2018reinforcement} or by \textbf{expert demonstrations} \cite{osa2018algorithmic}. 
\newline In the case of agent experience, there is a trial-and-error procedure where the control policy generates actions executed by an agent, which interacts with the environment. The parameters are then tuned according to the effectiveness of the actions, based on their impact on the environment relative to the task to be solved. 
\newline In the case of expert demonstrations, the control policy parameters are directly tuned using a dataset containing examples of task execution. Here, the goal is to replicate the tasks observed in the dataset.

% In questo caso, la policy di controllo non è addestrata per eseguire un singolo task (pick an object), con l'obiettivo di generalizzare rispetto a diversi oggetti e diverse condizioni iniziali. Ma è addestrata per eseguire diverse variazioni di uno specifico task (e.g., pick an object with different possible placing location) o addirittura task  diversi tra di loro (e.g., una singola policy di controllo che risolve sia il problema del pick-place sia problemi di assemblaggio), con l'obiettivo di generalizzare rispetto agli oggetti che vengono manipolati, le condizioni iniziali e rispetto ai task stessi. Questo significa che, sfruttando l'ipotesi di knowledge-sharing possiamo ottenere un sistema in grado di risolvere nuove variazioni.  
Given this background, the thesis is framed in the context of \textit{Learning from Demonstration} (LfD), a learning approach based on expert demonstrations. According to the requirements of adaptability the thesis focus on a specific aspect of LfD, named \textit{Multi-Task LfD}. In this case, the control policy is not trained to execute a single task (e.g., picking an object) with the goal of generalizing across different objects and initial conditions \cite{zhang2018deep_vr_teleoperation,mandlekar2022matters}. Instead, it is trained to handle various variations of a specific task (e.g., picking an object from different possible locations) \cite{dasari2021transformers_one_shot} or even entirely different tasks (e.g., a single control policy that solves both picking and placing tasks as well as assembly tasks) \cite{brohan2022rt,mandi2022towards_more_generalizable_one_shot}. The goal is to generalize not only with respect to the objects being manipulated and the initial conditions but also with respect to the tasks themselves. This means that it is possible to achieve a system capable of solving new variations by leveraging the knowledge-sharing hypothesis.

In this scenario, the learning procedure is much more challenging because there is the need to include and define the \textbf{conditioning signal} (i.e., the signal that informs the policy about the task to execute, the object to manipulate, and the target placing location). Additionally, the environment can contain \textbf{multiple distractor objects} (e.g., objects that can potentially be manipulated but are not of interest for a given task variation). 

Regarding the conditioning signal, there are at least two intuitive approaches. The first is through a natural language description of the task to be executed \cite{stepputtis2020language,mees2022calvin,brohan2022rt}, and the second is through a video demonstration \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}. 
\newline In the former, the task is described using phrases that specify the task details, such as ``Pick the red box and place it into the first bin". Given in input the phrase, the system must be able to infer the intent of the task (i.e., the pick-place operation) and the object of interest (e.g., red-box for picking and first bin for placing), and correlate this information with the environment and robot state to effectively control the robot.
\newline In the latter, another agent (either a robot or a human operator) performs the task in a different environment configuration, records this execution, and provides the video as input to the control policy. The control policy must then infer the intent from the video (i.e., the task to be performed, the object to be manipulated, and the final state) and control the robot to complete the task according to the agent's state, the environment's state, and the commanded task.

Inspired by how humans can learn to replicate tasks by simply observing their execution, the main goal of this thesis is to develop a system capable of replicating tasks shown in a video demonstration. This involves addressing challenges related to extracting task-relevant information from the video, such as identifying the manipulated object and its final position.

% Per quanto riguarda il problema legato alla presenza degli oggetti distrattori, in generale questi sono oggetti che non vengono mai considerati in operazioni di manipolazione, semplificando di molto il problema. Però nel contesto proposto in questa tesi il problema è ulteriormente enfatizzato dal fatto che il significato semantico di oggetto di interesse o distrattore associato agli oggetti presenti nella scena è definito a run-time dal comando stesso. Questo significa che se la configurazione iniziale è composta da 4 oggetti (e.g., 4 box di colore diverso), sulla base del comando dato al robot un determinato oggetto può diventare o meno di interesse. 
Regarding the issue of distractor objects, they are typically defined as items present in the scene but never involved in manipulation operations. Modern deep architectures can handle this scenario effectively, as they can easily learn to ignore these objects since they do not participate in any manipulation tasks. However, in the context proposed in this thesis, the problem is further complicated by the fact that the semantic meaning of an object (i.e., target or distractor) is defined at run-time by the command itself. This means that if the initial configuration consists of four objects (e.g., four boxes of different colors), a specific object may or may not be of interest based on the command given to the robot.

The primary contribution of this thesis is tackling the challenge of distractor objects. A key issue identified in existing literature is \textbf{target misidentification}, where the learned control policy generates valid trajectories, enabling the robot to reach, pick, and place objects, but frequently manipulates the wrong object.

To solve this problem, two main considerations were made:

\begin{enumerate}[label=\textbf{(\arabic*)}]
    \item Architectures proposed in the current literature are predominantly \textbf{end-to-end}, translating high-dimensional inputs, such as images, into corresponding low-dimensional actions. As a result, the model must learn an implicit representation that encodes both the task objective and the current state of the environment, including the location of the target object.
    \item The learning procedure optimizes an \textbf{action-centric \\ metric}, meaning that it is not directly linked to task success but instead focuses on mimicking the expert's actions on average. This action-focused optimization can lead to poor encoding of critical information, such as object positions.
\end{enumerate}

These two factors can result in a control policy that fails to effectively guide the robot toward the target object. In particular, it was observed that the early stages of trajectory execution are critical. Even small errors during these initial steps can cause the robot to reach and ultimately pick the wrong object.

Based on these considerations, this thesis explores the development of a \textbf{modular} architecture instead of an end-to-end approach. This architecture features modules specifically designed for reasoning about the objects of interest (e.g., target object and placement location). The outputs of these reasoning modules are then integrated to simplify the learning problem for the Control Module. This module is now informed by low-dimensional information, such as the position of the target object, which may be more effectively utilized during the learning process, especially in light of the action-centric cloning loss.

To perform this explicit reasoning, a \textit{Conditioned Object Detector} (COD) has been developed. This module, given the video demonstration and the current agent observation as input, predicts the category-agnostic bounding box related to the target object and the final placing location. This low-level positional information is then provided to the control module, which predicts the actions to perform.

The learning procedure is then divided into two steps. The first step involves training the \textit{Conditioned Object Detector} (COD) module, which focuses on explicitly solving cognitive tasks, such as detecting regions of interest represented by the object to be manipulated and its final location. The second step involves training the \textit{Object Conditioned Control Policy} (OCCP), which focuses on solving the control problem using low-level positional information that can be easily mapped into the corresponding actions.

The final system has been extensively tested in simulation environments, then it was also validated on a real-world robotic platform. 

Regarding the simulated environment, the system was evaluated on both \textbf{multi-variation single-task} scenarios and \\ \textbf{multi-variation multi-tasks} scenarios, considering four simulated tasks: Pick-Place, Nut-Assembly, Stack-Block, and \\ Button-Press. Each task had different variations based on the manipulated object and the final state. While the tasks share common properties, they also have specific characteristics. For example, the Nut-Assembly task involves contact-rich, precise manipulation, whereas Pick-Place can be solved in a much rougher manner.

Overall, the proposed methods demonstrated very promising behaviors and a general improvement over baseline methods that do not include object-related reasoning. This shows that solving manipulation tasks with an object-oriented approach can be an effective paradigm for LfD problems. Additionally, this approach provides interpretable information to the end user, as the predicted bounding boxes can be interpreted as the locations where the robot will move.

In conclusion, the proposed method was tested also in a real-world environment, where the complexity of the problem is heightened by the presence of less and noisy data collected through teleoperation. Even under these challenging conditions, the proposed method demonstrated its effectiveness in addressing both the cognitive and control problems. This confirms that the object prior can be successfully applied in real-world scenarios, enabling the development of a reliable system despite limited and noisy trajectory data.

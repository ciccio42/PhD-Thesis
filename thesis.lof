\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Industrial Robots: example of applications\relax }}{4}{figure.caption.3}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Examples of direct demonstration\relax }}{12}{figure.caption.5}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Examples of indirect demonstration\relax }}{14}{figure.caption.7}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Taxonomy of LfD methods, divided based on type of demonstration and the learning algorithm used to learn the learner policy $\pi ^{L}$ \relax }}{16}{figure.caption.9}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces Graphical representation of the idea behind VMP \cite {zhou2019learning}. The final trajectory \( y \) is represented as the sum of two components: the elementary trajectory \( h \), and the shape modulation \( f \). The elementary trajectory can directly connect two points (e.g., start and goal points) with a linear segment.\relax }}{22}{figure.caption.11}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Architecture proposed in\nobreakspace {}\cite {zhang2018deep_vr_teleoperation}\relax }}{25}{figure.caption.13}%
\contentsline {figure}{\numberline {1.7}{\ignorespaces The set of tasks presented in the benchmark \cite {mandlekar2022matters}.\relax }}{26}{figure.caption.15}%
\contentsline {figure}{\numberline {1.8}{\ignorespaces Architecture presented in \cite {cheng2023diffusion}. (a) General formulation, at time step $t$, the policy inputs the latest $T_o$ steps of observation data $O_t$ and outputs $T_a$ steps of actions $A_t$. (b) CNN-based Diffusion Policy, the observation feature $O_t$ is conditioned using FiLM \cite {perez2018film}. Starting with $A_t^K$ from Gaussian noise, the noise-prediction network $\epsilon _\theta $ iteratively subtracts noise to obtain the denoised action sequence $A_t^0$. (c) Transformer-based Diffusion Policy, the observation embedding $O_t$ is fed into a multi-head cross-attention layer within each decoder block, with causal attention applied to constrain each action embedding to attend only to itself and prior actions.\relax }}{30}{figure.caption.17}%
\contentsline {figure}{\numberline {1.9}{\ignorespaces Architecture proposed in\nobreakspace {}\cite {das2021model_based_irl_from_vd}\relax }}{39}{figure.caption.19}%
\contentsline {figure}{\numberline {1.10}{\ignorespaces The performance comparison proposed in \cite {ho2016gail} is presented here. The y-axis shows the scaled reward, where the expert's reward is set to 1 and the random baseline is set to 0. The IRL baselines FEM and GTAL refer to the IRL algorithm described in \cite {ho2016model}, but with different cost functions.\relax }}{42}{figure.caption.20}%
\contentsline {figure}{\numberline {1.11}{\ignorespaces Tasks solved in\nobreakspace {}\cite {zolna2021task_relevant_ail}\relax }}{45}{figure.caption.22}%
\contentsline {figure}{\numberline {1.12}{\ignorespaces Experimental results on tasks without and with spurious features\nobreakspace {}\cite {zolna2021task_relevant_ail}\relax }}{46}{figure.caption.23}%
\contentsline {figure}{\numberline {1.13}{\ignorespaces Representation of \textbf {embodiment mismatch problem}. (Left) The source domain represented by a video of human performing a task. (Right) The target domain, represented by the robot that executes the observed task\relax }}{47}{figure.caption.24}%
\contentsline {figure}{\numberline {1.14}{\ignorespaces Examples of how the mismatch between demonstrator viewpoint and learner viewpoint can be handled.\relax }}{51}{figure.caption.25}%
\contentsline {figure}{\numberline {1.15}{\ignorespaces Learning from Observation taxonomy\relax }}{52}{figure.caption.26}%
\contentsline {figure}{\numberline {1.16}{\ignorespaces Architecture proposed by\nobreakspace {}\cite {xiong2021learning_by_watching}\relax }}{53}{figure.caption.28}%
\contentsline {figure}{\numberline {1.17}{\ignorespaces Experimental results reported in \cite {torabi2018gaifo}.\relax }}{55}{figure.caption.30}%
\contentsline {figure}{\numberline {1.18}{\ignorespaces Representation of the learning procedure proposed by\nobreakspace {}\cite {torabi2018bco}\relax }}{57}{figure.caption.32}%
\contentsline {figure}{\numberline {1.19}{\ignorespaces DEAILO: (\ref {fig:dealio_task}) Control Tasks, (\ref {fig:dealio_performance}) Performance Level\relax }}{59}{figure.caption.33}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Robotic vision framework proposed in \cite {devin2018deep}. The framework is divided into different stages: \textbf {Meta-Attention}: Generates object proposals from an input image, trained on an object detection dataset, and shared across tasks; \textbf {Task-Specific Attention}: Focuses on relevant objects for a task using the meta-attention's semantic features; \textbf {Soft Attention}: Distributes attention as probabilities over object proposals using a Boltzmann distribution; \textbf {Movement Prediction Network}: Combines attended object information with the robot's state to predict the next action. \relax }}{65}{figure.caption.34}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Pouring task setting proposed in \cite {devin2018deep}. (Left) Mugs used for evaluation. Note that only the brown mug was seen during training. Center: Successful pouring into the pink mug. (Right) Pouring into the brown mug in a cluttered environment that was not seen during training.\relax }}{66}{figure.caption.35}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces The region proposals (meta-attention) are drawn in blue and the task-specific attended regions are drawn in red. For the Pouring task with distractor mug (pink) and target mug (brown), the attention locks on to the brown mug as its position defines the trajectory. For the sweeping task, two attention vectors are used, one attends to the orange and one attends to the dustpan.\relax }}{67}{figure.caption.36}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces The VIOLA architecture proposed in \cite {zhu2023viola}. (Top) The overall control architecture is based on a Transformer module that processes a stack of \textit {per-step features} $h_{t}$, obtained from the Feature Encoder, to generate a final action embedding, which is then input into a GMM policy. (Bottom) The Feature Encoder builds both local and global features. Local features correspond to regions of interest extracted by the RPN. Global features are obtained by processing the workspace image, the image from the camera on the gripper, and proprioceptive information. \relax }}{68}{figure.caption.37}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Simulation tasks on which the VIOLA \cite {zhu2023viola} method was tested. (Left) Sorting task. (Center) Stacking task. (Right) BUDS-Kitchen task\relax }}{69}{figure.caption.38}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces PLATO architecture proposed in \cite {belkhale2023plato}. The architecture is composed of different stages. (1) The posterior encoder \( E \) encodes the interaction sequence \( \tau ^{(i)} \) into the affordance \( z \). (2) The prior encoder \( E' \) encodes the object initial state \( o^{(i)}_1 \) and goal state \( o_g \) to predict \( z \), with \( o_g \) sampled after the interaction. (3) The policy is trained to output actions during the interaction period conditioned on the affordance. Simultaneously, (4) it is trained to output actions during the pre-interaction period conditioned on the ``future" affordance. \relax }}{71}{figure.caption.39}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Testing scenarios and primitives proposed in \cite {belkhale2023plato}. (Top) \textbf {Block2D} Environment primitive examples. (Center) \textbf {Block3D} and \textbf {Block3DPlatform} primitive examples. (Bottom) The left image shows an example primitive in \textbf {Mug3D-Platforms}. The right three images show sample tasks from \textbf {Playroom3D}.\relax }}{72}{figure.caption.40}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Example of Vision-Question Answering problem get from the CLEVR \cite {johnson2017clevr} dataset. It is possible to observe how for a given image multiple different questions can be done. As well as, questions covers different reasoning skills such as attribute identification, counting, comparison, multiple attention, and logical operations\relax }}{73}{figure.caption.41}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Proposed integration of the FiLM conditioning layer. Here, the Linear Modulation is applied to modify the activation maps of the ResNet blocks, while a linear layer generates the modulation coefficients $\beta $ and $\gamma $ based on the embedding derived from the textual prompt. This enables the model to conditionally adjust the activation maps according to the context provided by the input query\relax }}{75}{figure.caption.42}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Visualizations of the distribution of locations used by the model for its globally max-pooled features, from which its final MLP makes predictions. FiLM correctly localizes the object referenced by the answer (top) or all objects referenced by the question (bottom). However, the localization is less accurate when the model provides an incorrect answer (rightmost bottom).\relax }}{76}{figure.caption.43}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces Example of bounding-boxes assignmennt, where green boxes refer to target object and placing location while red boxed refers to no-target and no-target-place. (Left) The demonstrator manipulates the green box, placing it into the first bin. (Reight) The demonstrator manipulates the red box, placing it into the second bin. Note, how for a given agent environment state, the semantic attribute between objects changes.\relax }}{78}{figure.caption.44}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces The proposed \textit {Conditioned Object Detector} architecture takes as input the pair $(o^{a}_{t}, c_{m_{i}})$, where $o^{a}_{t}$ represents the agent observation and $c_{m_{i}}$ represents the demonstration frames. The agent observation is encoded using ResNet-18, while the demonstration frames are encoded through ResNet2+1. The FiLM conditioning layer is employed to inject information from the command $c_{m_{i}}$ into the feature maps extracted from the observation. Finally, Fast R-CNN generates the bounding boxes based on the conditioned input.\relax }}{80}{figure.caption.45}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces Examples of tasks and variations taken in consideration for the methods validation. The images report the final system state. For the pick-place, nut-assembly and stack-block tasks the variation is defined with respect to the target object and the placing location, while for the press-button the variation is defined according to the button to press.\relax }}{83}{figure.caption.46}%
\contentsline {figure}{\numberline {2.14}{\ignorespaces Example of predictions generated by the CTOD module in the single-task scenario. The blue-box is the predicted one, while the green is the ground truth.\relax }}{88}{figure.caption.50}%
\contentsline {figure}{\numberline {2.15}{\ignorespaces Example of Target Bounding Box and Final Placing Position Definition for the Press-Button Task. In this task, the target bounding box is represented by the green bounding box, which encloses the button that needs to be pressed. The blue bounding box represents the final placing position, indicating the location where the robot's end-effector should be positioned to successfully press the button.\relax }}{90}{figure.caption.52}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Multi-Task Imitation Learning Taxonomy\relax }}{96}{figure.caption.57}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Diagram of MAML algorithm, which optimizes for a representation $\theta $ that can quickly adapt to new tasks\relax }}{98}{figure.caption.58}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Tasks performed in \cite {yu2018daml}. (Top row) Human demonstration, (Bottom row) robot demonstration. (Left) Placing task, (Middle) pushing task, (Right) pick-and-place task.\relax }}{100}{figure.caption.59}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces The Temporal Adaptation Loss architecture applies 1D temporal convolutional layers to the stacked embeddings generated by the policy $\pi $ from the frames of the human video demonstration.\relax }}{101}{figure.caption.60}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Architecture proposed in \cite {stepputtis2020language}. The \textit {Semantic Model} takes in input the image $I$ and the command $v$, generating a command conditioned embedding $e$. The \textit {Control Module} receives in input the embedding $e$ and the current robot state $r_{t}$ and produces the next control signal.\relax }}{103}{figure.caption.61}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces (Left) Set of object used in \cite {stepputtis2020language}. (Right) Sample of task execution (right)\relax }}{103}{figure.caption.62}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Architecture proposed in \cite {jang2022bc_z}. Here, the Task Embedding is injected directly in the Feature Maps generated by the ResNet-18.\relax }}{105}{figure.caption.63}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Examples of household scenarios in RT-1 large-scale dataset.\relax }}{106}{figure.caption.64}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces RT-1 Language-Conditioned Transformer based architecture proposed in \cite {brohan2022rt}.\relax }}{106}{figure.caption.65}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Environment proposed in CALVIN benchmark \cite {calvin}. The environments have different textures and all static elements such as the sliding door, the drawer, the light button, and switch are positioned differently\relax }}{108}{figure.caption.68}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces The Multimodal Diffusion Transformer (MDT) architecture proposed in \cite {reuss2024multimodal}\relax }}{110}{figure.caption.69}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces The Dual Stream architecture proposed in \cite {shridhar2022cliport} consists of two parallel streams: a semantic stream and a spatial stream. The semantic stream utilizes a frozen CLIP ResNet50 to encode the RGB input, with the decoder layers conditioned by tiled language features from the CLIP sentence encoder. Meanwhile, the spatial stream encodes the RGB-D input, and its decoder layers are laterally fused with those of the semantic stream. The final output is a map of dense pixelwise features, which is used to predict pick or place affordances.\relax }}{111}{figure.caption.70}%
\contentsline {figure}{\numberline {3.13}{\ignorespaces Example of affordance map. (Left) The top-view input image. (Center) The affordance map for the pick-operation, since the task is to grab cherries the map highlights the two pickable cherries. (Right) The affordance map for the place operation.\relax }}{112}{figure.caption.71}%
\contentsline {figure}{\numberline {3.14}{\ignorespaces Architecture proposed in \cite {james2018task_embedded}. The \textit {Task Embedding Net} generates the embedding representing the task to be performed given the goal image. The \textit {Control Net} implements the policy, and takes in input the current observation and the tiled task embedding\relax }}{114}{figure.caption.72}%
\contentsline {figure}{\numberline {3.15}{\ignorespaces Transformer based architecture proposed in \cite {dasari2021transformers_one_shot}. The Transformer network is used to create task-specific representation, given context and observation features computed with ResNet-18.\relax }}{116}{figure.caption.73}%
\contentsline {figure}{\numberline {3.16}{\ignorespaces Validation setting proposed in \cite {dasari2021transformers_one_shot}. The 16 tasks consist of taking an object (a-b) to a bin (1-4). On the left there is the demonstrator robot, while on the right there is the agent robot.\relax }}{116}{figure.caption.74}%
\contentsline {figure}{\numberline {3.17}{\ignorespaces (Left) The TOSIL architecture, as proposed in \cite {dasari2021transformers_one_shot}. (Right) The MOSAIC architecture, as introduced in \cite {mandi2022towards_more_generalizable_one_shot}. In the MOSAIC architecture, the original encoder-decoder Transformer architecture has been replaced with an encoder-only architecture featuring self-attention modules.\relax }}{117}{figure.caption.75}%
\contentsline {figure}{\numberline {3.18}{\ignorespaces The evaluation tasks proposed in \cite {mandi2022towards_more_generalizable_one_shot} consist of a total of 7 tasks with 61 semantic variations.\relax }}{118}{figure.caption.76}%
\contentsline {figure}{\numberline {3.19}{\ignorespaces AWDA framework proposed in \cite {chang2023one}. The task inference network $f(v,o)$ predicts a sequence of attributed waypoints (red dots) that are achieved by hand-defined motion primitives. The original data are augmented with free-space motion trajectories and asymmetric demonstration mixup in order to reduce the correlation between tasks and task content.\relax }}{121}{figure.caption.78}%
\contentsline {figure}{\numberline {3.20}{\ignorespaces (Upper) General end-to-end architecture, where the \textit {Backbone Module} takes as input both the agent observation and the command. It generates an embedding $z_{t}$ that must contain information related to both the command and the control. (Bottom) In the modular architecture, there are two backbone modules: the \textit {Command Analysis Module}, which generates the task-embedding $z^{task}_t$, and the \textit {Backbone Module}, which is trained to generate only the control-embedding $z^{control}_{t}$.\relax }}{125}{figure.caption.80}%
\contentsline {figure}{\numberline {3.21}{\ignorespaces Proposed Single-Control Module Architecture. In contrast to the general architecture described in Figure \ref {fig:end_to_end_vs_modular}, the Command Analysis module is replaced by the CTOD module, which generates the bounding box related to the target object. The chosen backbone is the MOSAIC architecture \cite {mandi2022towards_more_generalizable_one_shot}. The control module is now informed by both low-level positional information ($bb^{target}_{t}$) and a control-oriented embedding ($z^{control}_{t}$), enabling it to make more informed decisions.\relax }}{128}{figure.caption.81}%
\contentsline {figure}{\numberline {3.22}{\ignorespaces Proposed Double-Control Module Architecture. In this architecture, the Control Module is split into two distinct modules, each responsible for learning a specific primitive: the \textit {reaching} primitive and the \textit {placing} primitive. The first module takes as input the bounding box corresponding to the target object ($bb^{target}_{t}$), while the second module receives the bounding box related to the final placing location ($bb^{placing}_{t}$). This separation allows for specialized control during both the reaching and placing phases.\relax }}{129}{figure.caption.82}%
\contentsline {figure}{\numberline {3.23}{\ignorespaces The distribution of trajectories for the different tasks along the x-y axis of the table workspace.\relax }}{130}{figure.caption.83}%
\contentsline {figure}{\numberline {3.24}{\ignorespaces Example of task rollout with incorrect object manipulation. In this scenario, the robot successfully completes the task by placing an object in the first bin. However, instead of manipulating the correct object (the green box), the robot mistakenly picks up the blue box. This illustrates a situation where the robot executes the task's final action correctly but selects the wrong object during manipulation.\relax }}{135}{figure.caption.86}%
\contentsline {figure}{\numberline {3.25}{\ignorespaces Example of unsuccessful Press-Button rollout. In this scenario, the robot successfully reaches the target button using the predicted bounding box (blue). However, due to instability in predictions during the pushing phase, the robot is unable to complete the pressing action, resulting in an unsuccessful task execution.\relax }}{137}{figure.caption.88}%
\contentsline {figure}{\numberline {3.26}{\ignorespaces Example of unsuccessful Pick-Place rollout. In this case, the robot fails to complete the task due to errors in the bounding box predictions. These inaccuracies cause the robot to move in the wrong direction, leading to an unsuccessful execution of the task.\relax }}{137}{figure.caption.89}%
\contentsline {figure}{\numberline {3.27}{\ignorespaces Example of an unsuccessful Pick-Place operation using the ``no-bb after pick" variant. In this scenario, the robot successfully reaches the target box based on the predicted bounding box (blue). However, the single-control module prematurely predicts the closing command, preventing the robot from correctly picking up the object.\relax }}{141}{figure.caption.95}%
\contentsline {figure}{\numberline {3.28}{\ignorespaces The dataset used for generalization tests removes one variation from each set of variations for a given target object.\relax }}{143}{figure.caption.98}%
\contentsline {figure}{\numberline {3.29}{\ignorespaces Proprioceptive information is integrated in both the end-to-end architecture (top) and the modular architecture (bottom). The proprioceptive vector, $x^{prop}$, is constructed from the robot's six continuous joint positions and the binary gripper state.\relax }}{144}{figure.caption.99}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Workspace comparison between real-world (left) and simulated (right) scenarios. Images are taken from the frontal camera (Top-Left), the gripper camera (Top-Right), lateral-left camera (Bottom-Left) and lateral-right (Bottom-Right).\relax }}{150}{figure.caption.104}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Set of variations used in the real-world robot evaluation. For each variation, the first and last frames are provided\relax }}{151}{figure.caption.105}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Placement configuration used for the trajectory collection. For each placement configuration, 4 trajectories were collected. The target object initially starts at the rightmost position, and in each subsequent trajectory, the box is moved to the adjacent position. This process is then repeated twice, each time with a different object orientation, resulting in a total of 40 trajectories for a given configuration.\relax }}{152}{figure.caption.106}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces (Left) Trajectory distribution along the x-y axis of the real-world dataset. (Right) Trajectory distribution along the x-y axis of the simulated dataset, constrained to the same variations and number of trajectories as the real-world counterpart. It can be observed that the real-world dataset exhibits a much sparser and noisier distribution, due to the fact that the trajectories are collected via teleoperation.\relax }}{154}{figure.caption.107}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces (Top Row) Example of CTOD prediction. The image shows one of the system least accurate predictions. While it successfully identifies the target object (blue bounding box), the prediction lacks precision compared to the ground truth (green bounding box). As shown, the offset error increases during placement, likely due to the object being occluded by the gripper. Additionally, some inaccuracies in the ground truth result from the automatic generation process.. (Bottom Row) \relax }}{157}{figure.caption.111}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces An example of a collision with the target object. This represents the most significant and relevant error mode in the proposed real-world system. As observed, the COD module successfully identifies both the target object and its intended placement location. However, the control module fails to complete the pick operation due to a collision.\relax }}{159}{figure.caption.114}%
\addvspace {10\p@ }
